{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbd7ed2-899b-4e24-b672-dd53f9042fbe",
   "metadata": {},
   "source": [
    "# Lab Three: Extending Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf9269-5de8-4055-b507-2f1acfb98452",
   "metadata": {},
   "source": [
    "#### Garrett Webb, Jackson Heck, and Maria Harrison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db4d12-3232-4280-9bb9-fea68abd9888",
   "metadata": {},
   "source": [
    "### NOTE: As this code was run multiple times, the outputs are slightly varied due to new fits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97c3f0-d6ac-45b4-8b2f-d24b1c6907b7",
   "metadata": {},
   "source": [
    "## 1. Preparation and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149aa46f-b416-4a32-8c63-6733ea791eeb",
   "metadata": {},
   "source": [
    "### 1.1 Explaining Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b1fea-0888-4de9-a40e-5ba89749b4a0",
   "metadata": {},
   "source": [
    "The dataset we are using is a subset of the data collected during the 1987 National Indonesia Contraceptive Prevalence Survey. The dataset contains 1473 samples and 9 features, including numerical and categorical data. \n",
    "\n",
    "Each sample correlates to a woman and her answers to the survey. The different features highlight factors like age, education level, religious views, occupation, level of media exposure, and standard of living. The target is the type of contraceptive method used. The three values for this are \"no use,\" \"long-term use,\" and \"short-term use.\" We will be using logistic regression to predict the type of contraceptive method used based on the given values of the 9 features. To do this, we will first split our data into training and testing data. \n",
    "\n",
    "The use of contraceptives can correlate to the rate of population growth. Thus, governining bodies might be interested in our model and results to help guide their policy decisions. The results from our model might help government officials understand what factors play a key role in determining the use of contraceptives. \n",
    "\n",
    "Since this model would most likely be used by governments or institutions related to health care, the model might be used mostly for offline analysis. This would be most likely determined by the privacy laws and health information privacy laws of the area using the model. \n",
    "\n",
    "Since governments and health care institutions do not like to make errors, our prediction model should be at least 90% accurate in order to be useful to the third parties interested in our model. This level of accuracy would allow our third parties to confidently use our model in making policy decisions.\n",
    "\n",
    "============================================================================================================================================================================\n",
    "\n",
    "\n",
    "Dataset: Contraceptive Method Choice Data Set URL: https://archive.ics.uci.edu/ml/datasets/Contraceptive+Method+Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87c3c3-1e49-4b1c-9b2c-1f5ff7e9c009",
   "metadata": {},
   "source": [
    "### 1.2 Define and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e117f7-d23a-44da-94a1-a0da72753bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wife_age</th>\n",
       "      <th>wife_education_level</th>\n",
       "      <th>husband_education_level</th>\n",
       "      <th>num_children</th>\n",
       "      <th>isMuslim</th>\n",
       "      <th>isNotWorking</th>\n",
       "      <th>husband_occupation</th>\n",
       "      <th>standard_of_living</th>\n",
       "      <th>isNotExposedToMedia</th>\n",
       "      <th>contraceptive_method_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wife_age  wife_education_level  husband_education_level  num_children  \\\n",
       "0        24                     2                        3             3   \n",
       "1        45                     1                        3            10   \n",
       "2        43                     2                        3             7   \n",
       "3        42                     3                        2             9   \n",
       "4        36                     3                        3             8   \n",
       "\n",
       "   isMuslim  isNotWorking  husband_occupation  standard_of_living  \\\n",
       "0         1             1                   2                   3   \n",
       "1         1             1                   3                   4   \n",
       "2         1             1                   3                   4   \n",
       "3         1             1                   3                   3   \n",
       "4         1             1                   3                   2   \n",
       "\n",
       "   isNotExposedToMedia  contraceptive_method_used  \n",
       "0                    0                          1  \n",
       "1                    0                          1  \n",
       "2                    0                          1  \n",
       "3                    0                          1  \n",
       "4                    0                          1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracies = []\n",
    "times = []\n",
    "\n",
    "df = pd.read_csv('./data/cmc.data')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b1e28b4-037a-4be2-8988-839cd95c2832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1473 entries, 0 to 1472\n",
      "Data columns (total 10 columns):\n",
      " #   Column                     Non-Null Count  Dtype\n",
      "---  ------                     --------------  -----\n",
      " 0   wife_age                   1473 non-null   int64\n",
      " 1   wife_education_level       1473 non-null   int64\n",
      " 2   husband_education_level    1473 non-null   int64\n",
      " 3   num_children               1473 non-null   int64\n",
      " 4   isMuslim                   1473 non-null   int64\n",
      " 5   isNotWorking               1473 non-null   int64\n",
      " 6   husband_occupation         1473 non-null   int64\n",
      " 7   standard_of_living         1473 non-null   int64\n",
      " 8   isNotExposedToMedia        1473 non-null   int64\n",
      " 9   contraceptive_method_used  1473 non-null   int64\n",
      "dtypes: int64(10)\n",
      "memory usage: 115.2 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# find data types\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f2ab52-bb29-4cbb-8449-3a70096ebad6",
   "metadata": {},
   "source": [
    "In practice, we will change the categorical features type to object. However, this is not necessarily necessary since we will eventually normalize the data changing all the feature types to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e96a05-7a65-447a-a4af-ec836ec54172",
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_features = ['isMuslim','isNotWorking','isNotExposedToMedia'];\n",
    "df[categ_features] = df[categ_features].astype(np.object_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9671105-c892-4aaa-9924-6c4502efb9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1473 entries, 0 to 1472\n",
      "Data columns (total 10 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   wife_age                   1473 non-null   int64 \n",
      " 1   wife_education_level       1473 non-null   int64 \n",
      " 2   husband_education_level    1473 non-null   int64 \n",
      " 3   num_children               1473 non-null   int64 \n",
      " 4   isMuslim                   1473 non-null   object\n",
      " 5   isNotWorking               1473 non-null   object\n",
      " 6   husband_occupation         1473 non-null   int64 \n",
      " 7   standard_of_living         1473 non-null   int64 \n",
      " 8   isNotExposedToMedia        1473 non-null   object\n",
      " 9   contraceptive_method_used  1473 non-null   int64 \n",
      "dtypes: int64(7), object(3)\n",
      "memory usage: 115.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e653685-e18f-40b4-b0e8-fd70635052a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wife_age\n",
      "[24 45 43 42 36 19 38 21 27 44 26 48 39 37 46 40 29 31 33 25 28 47 32 49\n",
      " 34 20 22 30 23 35 41 17 18 16]\n",
      "wife_education_level\n",
      "[2 1 3 4]\n",
      "husband_education_level\n",
      "[3 2 4 1]\n",
      "num_children\n",
      "[ 3 10  7  9  8  0  6  1  2  4  5 12 11 13 16]\n",
      "isMuslim\n",
      "[1 0]\n",
      "isNotWorking\n",
      "[1 0]\n",
      "husband_occupation\n",
      "[2 3 1 4]\n",
      "standard_of_living\n",
      "[3 4 2 1]\n",
      "isNotExposedToMedia\n",
      "[0 1]\n",
      "contraceptive_method_used\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "for col in df:\n",
    "    print(col)\n",
    "    print(df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd251e4-369c-4758-836a-660f597b9e6b",
   "metadata": {},
   "source": [
    "For education level, 1 refers to a low level of education while a 4 refers to a high level of education.\n",
    "\n",
    "For the occupation of the husband, a 1 refers to a job in the \"professional, technical, or clerical\" field, a 2 refers to a job in the \"sales or service\" field, a 3 refers to a job in the \"manual\" field, and a 4 refers to a job in the \"agricultural\" field.\n",
    "\n",
    "For the target, the contraceptive method used, a 1 refers to \"no use,\" a 2 refers to \"long-term use,\" and a 3 refers to \"short-term use.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a7076-2b77-4346-b590-4f54af67b499",
   "metadata": {},
   "source": [
    "From the above outputs, we can see that there are no missing or strange values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9517a6a7-e3fa-4673-8d33-d3a0ce053189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the number of duplicates\n",
    "len(df[df.duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b672d000-ad19-4406-99c0-c8c80421b357",
   "metadata": {},
   "source": [
    "This shows that 48 samples are duplicates. Since there over 1,000 samples and only 10 attributes, it is likely that some women will have the same answers to the survey resulting in the duplicate data. Unfortunately, it is difficult to determine if they are true duplicates since the dataset does not include any uniquely identifying information about each individual such as name, date of birth, or single identity number from an Indonesian identiy card.\n",
    "\n",
    "Since there are no missing values and not enough information to verify the duplicates, there is no need to impute any missing information or drop any instances or features. Additionally, we believe all features might be useful to predict the target and give us information on what factors in to choosing a contraceptive method. If we could verify that the duplicates were the answers of the same woman, then we would drop the duplicates in order to ensure that there is only one instance of each woman who answered the survey. If there were missing values, then we could use different methods of imputation depending on what type of data was missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fc18802-fc75-4e50-af32-379042e34125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    629\n",
      "3    511\n",
      "2    333\n",
      "Name: contraceptive_method_used, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_target = df['contraceptive_method_used'].value_counts()\n",
    "print(df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b936714c-acae-4585-89ce-541ea63e9cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of targets valued 1: 42.701968771215206\n",
      "Percentage of targets valued 2: 22.606924643584524\n",
      "Percentage of targets valued 3: 34.691106585200274\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of targets valued 1: \" + str(df_target[1]/len(df.index)*100))\n",
    "print(\"Percentage of targets valued 2: \" + str(df_target[2]/len(df.index)*100))\n",
    "print(\"Percentage of targets valued 3: \" + str(df_target[3]/len(df.index)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e5d77-23d1-406b-acc9-92f8d78de701",
   "metadata": {},
   "source": [
    "We will need to normalize the age data, but this will be done after dividing the data into the training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ba8e0-1a57-4700-b006-1807b8316870",
   "metadata": {},
   "source": [
    "### 1.3 Divide Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ffc7cd2-f682-4997-882c-cc8595cd39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['wife_age', 'wife_education_level', 'husband_education_level', 'num_children', 'isMuslim', 'isNotWorking', 'husband_occupation', 'standard_of_living', 'isNotExposedToMedia']].copy()\n",
    "y = df['contraceptive_method_used'].to_numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e07eed2-5642-4833-8bda-880c43eab01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1473 entries, 0 to 1472\n",
      "Data columns (total 9 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   wife_age                 1473 non-null   int64 \n",
      " 1   wife_education_level     1473 non-null   int64 \n",
      " 2   husband_education_level  1473 non-null   int64 \n",
      " 3   num_children             1473 non-null   int64 \n",
      " 4   isMuslim                 1473 non-null   object\n",
      " 5   isNotWorking             1473 non-null   object\n",
      " 6   husband_occupation       1473 non-null   int64 \n",
      " 7   standard_of_living       1473 non-null   int64 \n",
      " 8   isNotExposedToMedia      1473 non-null   object\n",
      "dtypes: int64(6), object(3)\n",
      "memory usage: 103.7+ KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a12b61c-77a5-466b-8af8-eed8476f4491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1473,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d38f88c-0908-49cb-82eb-13b1ce730779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1178, 9) (1178,)\n",
      "(295, 9) (295,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4673cc9e-6e89-48ca-8c7f-787cfde84919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wife_age</th>\n",
       "      <th>wife_education_level</th>\n",
       "      <th>husband_education_level</th>\n",
       "      <th>num_children</th>\n",
       "      <th>isMuslim</th>\n",
       "      <th>isNotWorking</th>\n",
       "      <th>husband_occupation</th>\n",
       "      <th>standard_of_living</th>\n",
       "      <th>isNotExposedToMedia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1178 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      wife_age  wife_education_level  husband_education_level  num_children  \\\n",
       "1390        35                     1                        2             5   \n",
       "29          43                     1                        2             8   \n",
       "270         30                     4                        4             2   \n",
       "934         46                     4                        4             4   \n",
       "1179        23                     3                        2             0   \n",
       "...        ...                   ...                      ...           ...   \n",
       "763         22                     3                        4             3   \n",
       "835         37                     2                        4             3   \n",
       "1216        31                     4                        4             4   \n",
       "559         29                     4                        3             4   \n",
       "684         40                     4                        4             2   \n",
       "\n",
       "     isMuslim isNotWorking  husband_occupation  standard_of_living  \\\n",
       "1390        1            1                   3                   3   \n",
       "29          1            1                   2                   4   \n",
       "270         1            1                   1                   4   \n",
       "934         0            1                   1                   4   \n",
       "1179        1            1                   2                   2   \n",
       "...       ...          ...                 ...                 ...   \n",
       "763         1            1                   2                   3   \n",
       "835         1            1                   2                   3   \n",
       "1216        1            1                   3                   3   \n",
       "559         1            0                   3                   2   \n",
       "684         0            0                   1                   4   \n",
       "\n",
       "     isNotExposedToMedia  \n",
       "1390                   0  \n",
       "29                     0  \n",
       "270                    0  \n",
       "934                    0  \n",
       "1179                   0  \n",
       "...                  ...  \n",
       "763                    0  \n",
       "835                    0  \n",
       "1216                   0  \n",
       "559                    0  \n",
       "684                    0  \n",
       "\n",
       "[1178 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e67fcb0-bdc8-4220-86a7-c583f98abd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 1, ..., 2, 2, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "634fba30-d199-41a2-96bb-dfdc5eef8605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 percentage: 36.33276740237691\n",
      "1 percentage: 41.76570458404075\n",
      "2 percentage: 21.901528013582343\n"
     ]
    }
   ],
   "source": [
    "elements_count = {}\n",
    "for element in y_train:\n",
    "   if element in elements_count:\n",
    "      elements_count[element] += 1\n",
    "   else:\n",
    "      elements_count[element] = 1\n",
    "for key, value in elements_count.items():\n",
    "    percentage = value/len(y_train)*100\n",
    "    print(f\"{key} percentage: {percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46e508c-90b9-477c-9991-2244fc460b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 percentage: 46.440677966101696\n",
      "2 percentage: 25.423728813559322\n",
      "3 percentage: 28.135593220338983\n"
     ]
    }
   ],
   "source": [
    "elements_count = {}\n",
    "for element in y_test:\n",
    "   if element in elements_count:\n",
    "      elements_count[element] += 1\n",
    "   else:\n",
    "      elements_count[element] = 1\n",
    "for key, value in elements_count.items():\n",
    "    percentage = value/len(y_test)*100\n",
    "    print(f\"{key} percentage: {percentage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d613ae7-e8bf-4c7a-9d45-46d77c698165",
   "metadata": {},
   "source": [
    "These were the percentage of the frequency of each target value in the original dataset:\n",
    "\n",
    "Percentage of targets valued 1: 42.701968771215206%\n",
    "\n",
    "Percentage of targets valued 2: 22.606924643584524%\n",
    "\n",
    "Percentage of targets valued 3: 34.691106585200274%.\n",
    "\n",
    "The percentages of those same values roughly match the percentage of the values in the training and testing target set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50389861-e7f6-4c28-8b58-19fa622122e2",
   "metadata": {},
   "source": [
    "An 80/20 split is appropriate because, while the accuracy appears low, we have determined this to be a result of the features and target being less correlated than expected, and not (or minimally) due to the training set being too small. We did run some of the algorithms that will be seen below with different training and testing splits. However, there was not much improvement. \n",
    "\n",
    "We want to split the dataset so that both the training and testing datsets are good representations of the problem. However, this also means the original dataset needs to represent the problem domain well. Thus, we want data that has ennough combinations of input data to fully understand all possibilities. \n",
    "\n",
    "Honestly, splitting the dataset and implementing our own algorithms might perform better if we had much more data to train. Thus, it would be nice to test this on the entirety of the orginal dataset that our dataset came from. \n",
    "\n",
    "While we think there might not be a high correlation between the features and the target values, our implementation might not perform as well since our model might not have enough data in the training set to properly learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973fa06c-d900-4877-9c82-d6461b82864e",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432bd058-afe1-4d0f-8ca7-e98b56ece4f9",
   "metadata": {},
   "source": [
    "### 2.1 Custom One-Vs-All Logistic Regression Classifier Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92082c-293f-40db-858b-7d699769db34",
   "metadata": {},
   "source": [
    "Most of the source code below was taken from Dr. Larson's notebooks with our own additions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705cf15a-6212-4956-b92a-fc5477d72662",
   "metadata": {},
   "source": [
    "#### 2.1.1 BinaryLogisticRegressionBase Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48d213df-19c2-4960-8b74-a8993f118283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionBase:\n",
    "    # private: \n",
    "    def __init__(self, eta, optimization_technique, iterations=20, regularization=None, C1=0.001, C2=0.001):\n",
    "        self.eta = eta\n",
    "        self.optimization_technique = optimization_technique\n",
    "        self.iters = iterations\n",
    "        self.regularization = regularization\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abab4c1-f4f4-4722-8b27-c479cf23eacd",
   "metadata": {},
   "source": [
    "#### 2.1.2 BinaryLogisticRegression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3e5483f-2b07-427b-849d-04ab5fa49368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            # the actual update inside of sum\n",
    "            gradi = (yi - self.predict_proba(xi,add_bias=False))*xi \n",
    "            # reshape to be column vector and add to gradient\n",
    "            gradient += gradi.reshape(self.w_.shape) \n",
    "        \n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f691aa-197e-48e8-b956-2114bb932d2b",
   "metadata": {},
   "source": [
    "#### 2.1.3 VectorBinaryLogisticRegression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65b44aa1-a021-48d7-88d5-8d7ce1da7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "        \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        if self.optimization_technique == \"steepest ascent\":\n",
    "            ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "            gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "            \n",
    "            gradient = gradient.reshape(self.w_.shape)\n",
    "            \n",
    "            if self.regularization == \"L1\":\n",
    "                gradient[1:] += -self.C1 * self.w_[1:]\n",
    "            elif self.regularization == \"L2\":\n",
    "                gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "            elif self.regularization == \"both\":\n",
    "                gradient[1:] += -self.C1 * self.w_[1:]\n",
    "                gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "            return gradient\n",
    "        elif self.optimization_technique == \"stochastic gradient ascent\":\n",
    "            ## implement this technique\n",
    "            idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "            ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "            gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "\n",
    "            gradient = gradient.reshape(self.w_.shape)\n",
    "            \n",
    "            if self.regularization == \"L1\":\n",
    "                gradient[1:] += -self.C1 * self.w_[1:]\n",
    "            elif self.regularization == \"L2\":\n",
    "                gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "            elif self.regularization == \"both\":\n",
    "                gradient[1:] += -self.C1 * self.w_[1:]\n",
    "                gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "\n",
    "            return gradient\n",
    "        elif self.optimization_technique == \"Newton's method\":\n",
    "            g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "            hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C1 # calculate the hessian\n",
    "\n",
    "            ydiff = y-g # get y difference\n",
    "            gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "            \n",
    "            gradient = gradient.reshape(self.w_.shape)\n",
    "            \n",
    "            if self.regularization == \"L1\":\n",
    "                gradient[1:] += -self.C1 * self.w_[1:]\n",
    "            elif self.regularization == \"L2\":\n",
    "                gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "            elif self.regularization == \"both\":\n",
    "                gradient[1:] += -self.C1 * self.w_[1:]\n",
    "                gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "        \n",
    "            return pinv(hessian) @ gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd95093e-7cac-4241-9fd4-3ee472441316",
   "metadata": {},
   "source": [
    "#### 2.1.4 LogisticRegression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f2b3a86-74c7-4b29-856f-70a457cbf7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, optimization_technique, iterations=20, regularization=None, C1=0.001, C2=0.001):\n",
    "        self.eta = eta\n",
    "        self.optimization_technique = optimization_technique\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        self.regularization = regularization\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "            \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "      \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(eta=self.eta, optimization_technique=self.optimization_technique, iterations=self.iters, regularization=self.regularization, C1=self.C1, C2=self.C2)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.unique_[np.argmax(self.predict_proba(X),axis=1)] # take argmax along row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b81a5-e8c1-40e3-a670-c36af241b846",
   "metadata": {},
   "source": [
    "### 2.2 Train Custom Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c495f88-df7b-435f-97a2-518ce95e5827",
   "metadata": {},
   "source": [
    "#### 2.2.1 Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6136c08d-56e9-48d4-920b-3218826bdab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize training set\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_scaled = StandardScaler().fit(X_train).transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "929a4178-506e-4830-91a4-1a7df39dc68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33249213, -1.92679928, -1.75205672, ...,  0.98946061,\n",
       "        -0.12632575, -0.27531634],\n",
       "       [ 1.31814985, -1.92679928, -1.75205672, ..., -0.1657273 ,\n",
       "         0.89293271, -0.27531634],\n",
       "       [-0.28354394,  1.02478662,  0.70207267, ..., -1.3209152 ,\n",
       "         0.89293271, -0.27531634],\n",
       "       ...,\n",
       "       [-0.16033672,  1.02478662,  0.70207267, ...,  0.98946061,\n",
       "        -0.12632575, -0.27531634],\n",
       "       [-0.40675115,  1.02478662, -0.52499202, ...,  0.98946061,\n",
       "        -1.14558421, -0.27531634],\n",
       "       [ 0.94852821,  1.02478662,  0.70207267, ..., -1.3209152 ,\n",
       "         0.89293271, -0.27531634]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb51064e-d089-4618-a220-2d28f53c21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize testing set\n",
    "X_test_scaled = StandardScaler().fit(X_test).transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1df9e056-008f-4264-b8dc-e2540465a3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.75602692,  0.04037042,  0.68583331, ...,  1.0290115 ,\n",
       "        -0.18131908, -0.31089271],\n",
       "       [ 0.29324201,  0.04037042, -0.53296685, ..., -0.13404748,\n",
       "        -0.18131908,  3.21654377],\n",
       "       [ 0.64299833,  1.0328098 ,  0.68583331, ..., -1.29710645,\n",
       "         0.86748736, -0.31089271],\n",
       "       ...,\n",
       "       [ 0.40982745,  1.0328098 ,  0.68583331, ...,  1.0290115 ,\n",
       "         0.86748736, -0.31089271],\n",
       "       [-0.52285605,  0.04037042, -0.53296685, ..., -0.13404748,\n",
       "        -2.27893195, -0.31089271],\n",
       "       [-1.57212498, -0.95206897, -1.75176702, ...,  1.0290115 ,\n",
       "        -1.23012552,  3.21654377]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8beaf1e-2584-419d-9a0d-849e9458c453",
   "metadata": {},
   "source": [
    "After normalizing the data, we began to test our different optimization techniques and regularization terms. Once we tested all the techniques, we went back and tried to see if there was a particular class that was failing in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb4d4fe-72d9-47bf-9551-6ac1861fab73",
   "metadata": {},
   "source": [
    "#### 2.2.2 Using Steepest Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b7c71cd-cee7-4178-964f-a616e14e8668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[-0.35574323  0.62351132 -0.4933093  -0.0631525  -0.77643201  0.18506735\n",
      "  -0.03291566 -0.06159994 -0.25751917  0.13519648]\n",
      " [-1.48201729  0.12439111  0.62587193 -0.00978393  0.34014011 -0.15456979\n",
      "  -0.0975794  -0.14104424  0.18486156 -0.05237376]\n",
      " [-0.63814169 -0.77818519  0.07647448  0.0963148   0.55450497 -0.06347826\n",
      "   0.09682289  0.17125248  0.14530334 -0.13256418]]\n",
      "Accuracy: 52.12224108658744%\n",
      "CPU times: user 403 ms, sys: 1.15 s, total: 1.55 s\n",
      "Wall time: 106 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = LogisticRegression(0.1, optimization_technique=\"steepest ascent\", iterations=500)\n",
    "lr.fit(X_train_scaled,y_train)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X_train_scaled);\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_train,yhat)*100) + \"%\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0813db-f148-4f41-b407-667151ab79f6",
   "metadata": {},
   "source": [
    "We wanted to test different values for the learning rates with no regularization. Our research showed that the values below were the most commonly used values for learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151670e-742e-47ce-9c3c-883914580336",
   "metadata": {},
   "source": [
    "##### 2.2.2.1 Testing Different Learning Rates and Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "198b269a-6ab2-4d6f-ad52-b9dd3ec6f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "etas = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "484ea424-1331-43d2-a373-276c1f5e3b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.001: 48.38709677419355%\n",
      "Accuracy of 0.003: 48.38709677419355%\n",
      "Accuracy of 0.01: 48.302207130730054%\n",
      "Accuracy of 0.03: 50.0%\n",
      "Accuracy of 0.1: 53.140916808149406%\n",
      "Accuracy of 0.3: 52.54668930390493%\n",
      "Best accuracy with eta=0.1: 53.140916808149406%\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "best_eta = 0\n",
    "for e in etas:\n",
    "    eta = e\n",
    "    lr = LogisticRegression(eta=eta, optimization_technique=\"steepest ascent\", iterations=100)\n",
    "    lr.fit(X_train_scaled,y_train)\n",
    "    yhat = lr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_eta = eta\n",
    "    print(\"Accuracy of \" + str(eta) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "    \n",
    "print(\"Best accuracy with eta=\" + str(best_eta) + \": \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b2de16-ccdd-4693-8ddc-01a3b8b99268",
   "metadata": {},
   "source": [
    "We tested these values with different values for the number of iterations. The combination of eta equaling 0.1 and the number of iterations equaling 100 is where we had the highest level of accuracy. This level of accuracy was about 53.14%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ddf5514-f63c-4342-b5be-e4dcee2ab516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.3: 53.140916808149406%\n",
      "Accuracy of 0.3: 47.45762711864407%\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(eta=0.1, optimization_technique=\"steepest ascent\", iterations=100)\n",
    "lr.fit(X_train_scaled,y_train)\n",
    "yhat_train = lr.predict(X_train_scaled)\n",
    "print(\"Accuracy of \" + str(eta) + \": \" + str(accuracy_score(y_train,yhat_train)*100) + \"%\")\n",
    "yhat_test = lr.predict(X_test_scaled)\n",
    "print(\"Accuracy of \" + str(eta) + \": \" + str(accuracy_score(y_test,yhat_test)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13453d3-3208-4d20-9e2e-8bfbfdcdf55e",
   "metadata": {},
   "source": [
    "We fit these exact parameters on our training data. Then we ran our predictions on our testing data. Our accuracy score was about 47.46%. This is roughly 5.5% less than the accuracy of our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abafd803-187a-40de-a414-243e93c7d5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "Total number of failed predictions: 552\n",
      "Percentage of failed predictions for training set: 46.859083191850594\n",
      "Percentage of class 1 prediction failure: 32.427536231884055\n",
      "Percentage of class 2 prediction failure: 29.347826086956523\n",
      "Percentage of class 3 prediction failure: 38.224637681159415\n",
      "TESTING SET\n",
      "Total number of failed predictions: 155\n",
      "Percentage of failed predictions for testing set: 52.54237288135594\n",
      "Percentage of class 1 prediction failure: 34.83870967741935\n",
      "Percentage of class 2 prediction failure: 33.5483870967742\n",
      "Percentage of class 3 prediction failure: 31.61290322580645\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "\n",
    "for i, j in zip(y_train,yhat_train):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TRAINING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for training set: \" + str(total/len(y_train)*100))\n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))\n",
    "\n",
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "for i, j in zip(y_test,yhat_test):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TESTING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for testing set: \" + str(total/len(y_test)*100))   \n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3906a4c1-51dc-4017-8710-93120acc2816",
   "metadata": {},
   "source": [
    "From this information, it doesn't appear that one class seemed to fail in predicting using steepest ascent logistic regression more than another class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b09cce81-2880-4fe3-9a47-e5ace1d81735",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b402b9e4-fbff-4199-bb83-657c41b6dbdb",
   "metadata": {},
   "source": [
    "##### 2.2.2.2 Testing Applying L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda10782-c7a6-4723-bb15-9dfeb16f5928",
   "metadata": {},
   "source": [
    "Next, we wanted to test L1 regularization. Thus we created an array of different cost values to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6861511-d9d3-4d25-945a-77527d0f86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_vals = np.logspace(-3,1,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a176baa-3377-4915-b081-6ddc017ebaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e-03 1.93069773e-03 3.72759372e-03 7.19685673e-03\n",
      " 1.38949549e-02 2.68269580e-02 5.17947468e-02 1.00000000e-01\n",
      " 1.93069773e-01 3.72759372e-01 7.19685673e-01 1.38949549e+00\n",
      " 2.68269580e+00 5.17947468e+00 1.00000000e+01]\n"
     ]
    }
   ],
   "source": [
    "print(cost_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3ce84-0f6b-47a3-9f97-fbe1394a5786",
   "metadata": {},
   "source": [
    "These cost values range from 0.001 to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "605c7047-e53e-49e7-8754-696beddcb946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.001: 52.46179966044142%\n",
      "Accuracy of 0.0019306977288832496: 52.46179966044142%\n",
      "Accuracy of 0.003727593720314938: 52.46179966044142%\n",
      "Accuracy of 0.0071968567300115215: 52.207130730050935%\n",
      "Accuracy of 0.013894954943731374: 52.29202037351443%\n",
      "Accuracy of 0.026826957952797246: 52.54668930390493%\n",
      "Accuracy of 0.0517947467923121: 52.207130730050935%\n",
      "Accuracy of 0.1: 51.273344651952456%\n",
      "Accuracy of 0.19306977288832497: 50.33955857385399%\n",
      "Accuracy of 0.3727593720314938: 49.830220713073004%\n",
      "Accuracy of 0.7196856730011514: 46.6893039049236%\n",
      "Accuracy of 1.3894954943731375: 42.69949066213922%\n",
      "Accuracy of 2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of 5.179474679231207: 41.76570458404075%\n",
      "Accuracy of 10.0: 41.76570458404075%\n",
      "Best accuracy with C1=0.026826957952797246: 52.54668930390493%\n",
      "CPU times: user 5.05 s, sys: 20 s, total: 25.1 s\n",
      "Wall time: 1.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_accuracy = 0\n",
    "best_C1 = 0\n",
    "for C in cost_vals:\n",
    "    C1 = C\n",
    "    lr = LogisticRegression(0.1, optimization_technique=\"steepest ascent\", iterations = 400, regularization=\"L1\", C1=C1)\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    yhat = lr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_C1 = C1\n",
    "    print(\"Accuracy of \" + str(C1) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "    \n",
    "print(\"Best accuracy with C1=\" + str(best_C1) + \": \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a33eb7-9b97-4100-835e-607dc49e8d02",
   "metadata": {},
   "source": [
    "The best accuracy we got with using L1 regularization had the parameters of eta equaling 0.1, the number of iterations equaling 400, and C1 equaling about 0.0268. Our accuracy in predicting on the training set equaled about 52.55%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9744f3c2-5c2a-433a-816d-a0ffe43c91af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C1=0.026826957952797246: 48.8135593220339%\n"
     ]
    }
   ],
   "source": [
    "C1 = 0.026826957952797246\n",
    "lr = LogisticRegression(eta=0.1, optimization_technique=\"steepest ascent\", iterations=400, regularization=\"L1\", C1=C1)\n",
    "lr.fit(X_train_scaled,y_train)\n",
    "yhat = lr.predict(X_test_scaled)\n",
    "print(\"Accuracy of C1=\" + str(C1) + \": \" + str(accuracy_score(y_test,yhat)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37bfce-849a-4aeb-9606-6818828460a9",
   "metadata": {},
   "source": [
    "We fit these exact parameters on our training data. Then we ran predict() on our test data. When comparing the accuracy of our predictions of the testing target values and the actual testing target values, we got about 48.81% accuracy. This was roughly 5% less than the accuracy on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "add2b9a9-114e-4bd0-ba5a-04bd36c79ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C1=0.026826957952797246: 52.207130730050935%\n",
      "Accuracy of C1=0.026826957952797246: 49.152542372881356%\n",
      "3.054588357169578\n"
     ]
    }
   ],
   "source": [
    "C1 = 0.026826957952797246\n",
    "lr = LogisticRegression(0.1, optimization_technique=\"steepest ascent\", iterations=300, regularization=\"L1\", C1=C1)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "yhat_train = lr.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train,yhat_train)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \": \" + str(accuracy_score(y_train,yhat_train)*100) + \"%\")\n",
    "\n",
    "yhat_test = lr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test,yhat_test)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \": \" + str(accuracy_score(y_test,yhat_test)*100) + \"%\")\n",
    "\n",
    "print(abs(train_accuracy-test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a82db3-84a8-41d7-8f49-0ce10502f6bd",
   "metadata": {},
   "source": [
    "However, if change the number of iterations to 300, the difference between the accuracy scores on the training and testing data is only about 3.05%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3115a93c-6a02-44e7-b800-af27277c0379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "Total number of failed predictions: 563\n",
      "Percentage of failed predictions for training set: 47.792869269949065\n",
      "Percentage of class 1 prediction failure: 30.905861456483123\n",
      "Percentage of class 2 prediction failure: 31.97158081705151\n",
      "Percentage of class 3 prediction failure: 37.12255772646537\n",
      "TESTING SET\n",
      "Total number of failed predictions: 150\n",
      "Percentage of failed predictions for testing set: 50.847457627118644\n",
      "Percentage of class 1 prediction failure: 34.0\n",
      "Percentage of class 2 prediction failure: 36.666666666666664\n",
      "Percentage of class 3 prediction failure: 29.333333333333332\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "\n",
    "for i, j in zip(y_train,yhat_train):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TRAINING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for training set: \" + str(total/len(y_train)*100))\n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))\n",
    "\n",
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "for i, j in zip(y_test,yhat_test):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TESTING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for testing set: \" + str(total/len(y_test)*100))   \n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e56456-aa4a-4220-ac2d-c9d5a026f14f",
   "metadata": {},
   "source": [
    "Once again, the output does not show us that there is one class that is failing more often. In the training set, predicting class 3 had more failure than the others. However, in the testing set, predicting class 3 had the least amount of failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ca5c1e-5732-4b48-b660-495f014820fc",
   "metadata": {},
   "source": [
    "##### 2.2.2.3 Testing Applying L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8395f-dbd8-4b39-b2b0-a08f60582652",
   "metadata": {},
   "source": [
    "Next, we tested applying L2 regularization. We used the same cost values to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68d33ff9-9ce1-4170-af73-cf066b24f15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.001: 53.140916808149406%\n",
      "Accuracy of 0.0019306977288832496: 53.140916808149406%\n",
      "Accuracy of 0.003727593720314938: 53.140916808149406%\n",
      "Accuracy of 0.0071968567300115215: 52.88624787775892%\n",
      "Accuracy of 0.013894954943731374: 52.71646859083192%\n",
      "Accuracy of 0.026826957952797246: 52.46179966044142%\n",
      "Accuracy of 0.0517947467923121: 52.46179966044142%\n",
      "Accuracy of 0.1: 51.273344651952456%\n",
      "Accuracy of 0.19306977288832497: 49.745331069609506%\n",
      "Accuracy of 0.3727593720314938: 46.6044142614601%\n",
      "Accuracy of 0.7196856730011514: 43.20882852292021%\n",
      "Accuracy of 1.3894954943731375: 41.76570458404075%\n",
      "Accuracy of 2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of 5.179474679231207: 41.76570458404075%\n",
      "Accuracy of 10.0: 25.46689303904924%\n",
      "Best accuracy with C2=0.001: 53.140916808149406%\n",
      "CPU times: user 1.82 s, sys: 7.65 s, total: 9.48 s\n",
      "Wall time: 620 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_accuracy = 0\n",
    "best_C2 = 0\n",
    "for C in cost_vals:\n",
    "    C2 = C\n",
    "    lr = LogisticRegression(0.1, optimization_technique=\"steepest ascent\", iterations = 100, regularization=\"L2\", C2=C2)\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    yhat = lr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_C2 = C2\n",
    "    print(\"Accuracy of \" + str(C2) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "    \n",
    "print(\"Best accuracy with C2=\" + str(best_C2) + \": \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac5c15-eb50-4e12-91e6-3fcc1bd1bad3",
   "metadata": {},
   "source": [
    "The parameters using L2 regularization that resulted in the highest level of accuracy were eta equaling 0.1, the number of iterations equaling 100, and the cost value equaling 0.001. This gave us an accuracy of about 53.14%. This accuracy was higher than the one achieved using L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db52f526-0b86-4c54-bb7f-9b896ab02f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C2=0.001: 47.45762711864407%\n"
     ]
    }
   ],
   "source": [
    "C2 = 0.001\n",
    "lr = LogisticRegression(0.1, optimization_technique=\"steepest ascent\", iterations=100, regularization=\"L2\", C2=C2)\n",
    "lr.fit(X_train_scaled,y_train)\n",
    "yhat = lr.predict(X_test_scaled)\n",
    "print(\"Accuracy of C2=\" + str(C2) + \": \" + str(accuracy_score(y_test,yhat)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47055eb0-c21a-4606-945d-f21fd83f63f9",
   "metadata": {},
   "source": [
    "We then fit these exact parameters on our training data and predicted our target values on the testing data. The accuracy was about 47.46%. This was about 6% less than the accuracy from our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc8a4491-877a-48ce-8431-bb03b769f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C2=0.001: 52.37691001697793%\n",
      "Accuracy of C2=0.001: 50.847457627118644%\n",
      "1.529452389859287\n",
      "CPU times: user 470 ms, sys: 1.65 s, total: 2.12 s\n",
      "Wall time: 135 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "C2 = 0.001\n",
    "lr = LogisticRegression(0.1, optimization_technique=\"steepest ascent\", iterations = 500, regularization=\"L2\", C2=C2)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "yhat_train = lr.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train,yhat_train)*100\n",
    "print(\"Accuracy of C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat_train)*100) + \"%\")\n",
    "\n",
    "yhat_test = lr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test,yhat_test)*100\n",
    "print(\"Accuracy of C2=\" + str(C2) + \": \" + str(accuracy_score(y_test,yhat_test)*100) + \"%\")\n",
    "\n",
    "print(abs(train_accuracy-test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b07787-4d24-4cba-bb62-664873572b5a",
   "metadata": {},
   "source": [
    "However, if we set the number of iterations to 500, the accuracy score from the training set matches more closely to the accuracy from the testing set. The difference between them becomes about 1.53%. This is what we are hoping for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32820586-b90b-45b7-90db-dc515649b8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "Total number of failed predictions: 561\n",
      "Percentage of failed predictions for training set: 47.62308998302207\n",
      "Percentage of class 1 prediction failure: 32.62032085561498\n",
      "Percentage of class 2 prediction failure: 31.372549019607842\n",
      "Percentage of class 3 prediction failure: 36.007130124777184\n",
      "TESTING SET\n",
      "Total number of failed predictions: 145\n",
      "Percentage of failed predictions for testing set: 49.152542372881356\n",
      "Percentage of class 1 prediction failure: 31.724137931034484\n",
      "Percentage of class 2 prediction failure: 37.93103448275862\n",
      "Percentage of class 3 prediction failure: 30.344827586206897\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "\n",
    "for i, j in zip(y_train,yhat_train):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TRAINING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for training set: \" + str(total/len(y_train)*100))\n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))\n",
    "\n",
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "for i, j in zip(y_test,yhat_test):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TESTING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for testing set: \" + str(total/len(y_test)*100))   \n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34db13-b46d-4a1c-9494-642af54dd8bd",
   "metadata": {},
   "source": [
    "There does not seem to be a clear decision on a particular class that is failing more often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffcb63c-2b0b-485f-89a7-8d33b5844635",
   "metadata": {},
   "source": [
    "##### 2.2.2.4 Testing Applying Both L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4013ac-3f7e-49b8-a7e5-01e4904ac315",
   "metadata": {},
   "source": [
    "Next, we wanted to test applying both L1 and L2 regularization. To do so, we found all the possible different combinations of length 2 from our cost values. Then we iterated through these combinations twice. The first time, we set C1 to the first value and C2 to the second value. During the second iteration, we swapped thier values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a8f02cf-c649-4710-a449-578346fba35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "def combinations_data(iter, length):\n",
    "    return it.combinations(iter, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab25cb11-2c63-4b1a-9bf0-aeff4b5d4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_vals_combos = combinations_data(cost_vals, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97073c3f-2f52-4968-b0b5-441c8e718aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C1=0.001 and C2=0.0019306977288832496: 52.37691001697793%\n",
      "Accuracy of C1=0.001 and C2=0.003727593720314938: 52.207130730050935%\n",
      "Accuracy of C1=0.001 and C2=0.0071968567300115215: 52.207130730050935%\n",
      "Accuracy of C1=0.001 and C2=0.013894954943731374: 52.46179966044142%\n",
      "Accuracy of C1=0.001 and C2=0.026826957952797246: 53.3106960950764%\n",
      "Accuracy of C1=0.001 and C2=0.0517947467923121: 52.71646859083192%\n",
      "Accuracy of C1=0.001 and C2=0.1: 51.95246179966044%\n",
      "Accuracy of C1=0.001 and C2=0.19306977288832497: 49.830220713073004%\n",
      "Accuracy of C1=0.001 and C2=0.3727593720314938: 47.02886247877759%\n",
      "Accuracy of C1=0.001 and C2=0.7196856730011514: 44.14261460101868%\n",
      "Accuracy of C1=0.001 and C2=1.3894954943731375: 41.76570458404075%\n",
      "Accuracy of C1=0.001 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.001 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.001 and C2=10.0: 28.01358234295416%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.003727593720314938: 52.207130730050935%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.0071968567300115215: 52.207130730050935%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.013894954943731374: 52.46179966044142%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.026826957952797246: 53.2258064516129%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.0517947467923121: 52.71646859083192%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.1: 51.95246179966044%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.19306977288832497: 49.830220713073004%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.3727593720314938: 46.859083191850594%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.7196856730011514: 44.14261460101868%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=1.3894954943731375: 41.76570458404075%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=10.0: 28.01358234295416%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.0071968567300115215: 52.29202037351443%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.013894954943731374: 52.37691001697793%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.026826957952797246: 53.2258064516129%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.0517947467923121: 52.801358234295414%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.1: 51.95246179966044%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.19306977288832497: 49.745331069609506%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.3727593720314938: 46.774193548387096%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.7196856730011514: 44.05772495755518%\n",
      "Accuracy of C1=0.003727593720314938 and C2=1.3894954943731375: 41.76570458404075%\n",
      "Accuracy of C1=0.003727593720314938 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.003727593720314938 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.003727593720314938 and C2=10.0: 27.928692699490664%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.013894954943731374: 52.46179966044142%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.026826957952797246: 53.140916808149406%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.0517947467923121: 52.97113752122241%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.1: 51.95246179966044%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.19306977288832497: 49.40577249575552%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.3727593720314938: 46.6893039049236%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.7196856730011514: 43.97283531409168%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=1.3894954943731375: 41.76570458404075%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=10.0: 28.098471986417657%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.026826957952797246: 53.0560271646859%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.0517947467923121: 53.0560271646859%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.1: 51.95246179966044%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.19306977288832497: 49.32088285229202%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.3727593720314938: 46.6044142614601%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.7196856730011514: 43.97283531409168%\n",
      "Accuracy of C1=0.013894954943731374 and C2=1.3894954943731375: 41.76570458404075%\n",
      "Accuracy of C1=0.013894954943731374 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.013894954943731374 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.013894954943731374 and C2=10.0: 27.504244482173174%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.0517947467923121: 52.88624787775892%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.1: 51.52801358234296%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.19306977288832497: 49.15110356536502%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.3727593720314938: 46.6893039049236%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.7196856730011514: 43.803056027164686%\n",
      "Accuracy of C1=0.026826957952797246 and C2=1.3894954943731375: 41.76570458404075%\n",
      "Accuracy of C1=0.026826957952797246 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.026826957952797246 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.026826957952797246 and C2=10.0: 27.079796264855688%\n",
      "Accuracy of C1=0.0517947467923121 and C2=0.1: 50.93378607809848%\n",
      "Accuracy of C1=0.0517947467923121 and C2=0.19306977288832497: 49.660441426146015%\n",
      "Accuracy of C1=0.0517947467923121 and C2=0.3727593720314938: 46.6893039049236%\n",
      "Accuracy of C1=0.0517947467923121 and C2=0.7196856730011514: 43.463497453310694%\n",
      "Accuracy of C1=0.0517947467923121 and C2=1.3894954943731375: 41.76570458404075%\n",
      "Accuracy of C1=0.0517947467923121 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.0517947467923121 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.0517947467923121 and C2=10.0: 25.891341256366722%\n",
      "Accuracy of C1=0.1 and C2=0.19306977288832497: 49.57555178268252%\n",
      "Accuracy of C1=0.1 and C2=0.3727593720314938: 46.774193548387096%\n",
      "Accuracy of C1=0.1 and C2=0.7196856730011514: 43.12393887945671%\n",
      "Accuracy of C1=0.1 and C2=1.3894954943731375: 41.76570458404075%\n",
      "Accuracy of C1=0.1 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.1 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.1 and C2=10.0: 25.042444821731745%\n",
      "Accuracy of C1=0.19306977288832497 and C2=0.3727593720314938: 46.26485568760611%\n",
      "Accuracy of C1=0.19306977288832497 and C2=0.7196856730011514: 43.12393887945671%\n",
      "Accuracy of C1=0.19306977288832497 and C2=1.3894954943731375: 41.76570458404075%\n",
      "Accuracy of C1=0.19306977288832497 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.19306977288832497 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.19306977288832497 and C2=10.0: 24.27843803056027%\n",
      "Accuracy of C1=0.3727593720314938 and C2=0.7196856730011514: 42.27504244482173%\n",
      "Accuracy of C1=0.3727593720314938 and C2=1.3894954943731375: 41.76570458404075%\n",
      "Accuracy of C1=0.3727593720314938 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.3727593720314938 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.3727593720314938 and C2=10.0: 23.429541595925297%\n",
      "Accuracy of C1=0.7196856730011514 and C2=1.3894954943731375: 41.76570458404075%\n",
      "Accuracy of C1=0.7196856730011514 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.7196856730011514 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.7196856730011514 and C2=10.0: 22.665534804753822%\n",
      "Accuracy of C1=1.3894954943731375 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=1.3894954943731375 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=1.3894954943731375 and C2=10.0: 23.00509337860781%\n",
      "Accuracy of C1=2.6826957952797246 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=2.6826957952797246 and C2=10.0: 22.920203735144312%\n",
      "Accuracy of C1=5.179474679231207 and C2=10.0: 22.75042444821732%\n",
      "Best accuracy with C1=0.001 and C2=0.026826957952797246: 53.3106960950764%\n",
      "CPU times: user 8.22 s, sys: 31.2 s, total: 39.5 s\n",
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "iterations = 80\n",
    "\n",
    "best_accuracy = 0\n",
    "best_C1 = 0\n",
    "best_C2 = 0\n",
    "\n",
    "for i in cost_vals_combos:\n",
    "    C1 = i[0]\n",
    "    C2 = i[1]\n",
    "    lr = LogisticRegression(0.1, optimization_technique=\"steepest ascent\", iterations = iterations, regularization=\"both\", C1=C1, C2=C2)\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    yhat = lr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_C1 = C1\n",
    "        best_C2 = C2\n",
    "    print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "\n",
    "for i in cost_vals_combos:\n",
    "    C1 = i[1]\n",
    "    C2 = i[0]\n",
    "    lr = LogisticRegression(0.1, optimization_technique=\"steepest ascent\", iterations = iterations, regularization=\"both\", C1=C1, C2=C2)\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    yhat = lr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_C1 = C1\n",
    "        best_C2 = C2\n",
    "    print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "    \n",
    "print(\"Best accuracy with C1=\" + str(best_C1) + \" and C2=\" + str(best_C2) + \": \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac16e851-f7b2-4246-a4d3-16e79290e298",
   "metadata": {},
   "source": [
    "Fitting and predicting on our training set, these were the parameters that gave us our best accuracy:\n",
    "\n",
    "eta = 0.1\n",
    "\n",
    "iterations = 80\n",
    "\n",
    "C1 = 0.001\n",
    "\n",
    "C2 = 0.026826957952797246.\n",
    "\n",
    "The resulting accuracy score was 53.31%. This is the highest accuracy we have gotten so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0adff1e-ffee-4745-abfa-ba54648c6ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C1=0.001 and C2=0.026826957952797246: 53.3106960950764%\n",
      "Accuracy of C1=0.001 and C2=0.026826957952797246: 48.47457627118644%\n",
      "4.836119823889959\n"
     ]
    }
   ],
   "source": [
    "C1 = 0.001\n",
    "C2 = 0.026826957952797246\n",
    "\n",
    "lr = LogisticRegression(0.1, optimization_technique=\"steepest ascent\", iterations = 80, regularization=\"both\", C1=C1, C2=C2)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "yhat = lr.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train,yhat)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "\n",
    "yhat = lr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test,yhat)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_test,yhat)*100) + \"%\")\n",
    "print(abs(train_accuracy-test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b3e79-1281-46a3-b276-58801fe67920",
   "metadata": {},
   "source": [
    "Fitting and predicting with these parameters on our testing data, we scored an accuracy of about 48.47%. The difference between the accuracy scores was about 4.84%. But can we get closer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abc6feec-846e-4d92-9b52-ea0cb6c86095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C1=0.001 and C2=0.026826957952797246: 52.12224108658744%\n",
      "Accuracy of C1=0.001 and C2=0.026826957952797246: 48.8135593220339%\n",
      "3.3086817645535334\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "C1 = 0.001\n",
    "C2 = 0.026826957952797246\n",
    "\n",
    "iterations = 400\n",
    "\n",
    "lr = LogisticRegression(0.1, optimization_technique=\"steepest ascent\", iterations = iterations, regularization=\"both\", C1=C1, C2=C2)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "yhat_train = lr.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train,yhat_train)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat_train)*100) + \"%\")\n",
    "\n",
    "yhat_test = lr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test,yhat_test)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_test,yhat_test)*100) + \"%\")\n",
    "print(abs(train_accuracy-test_accuracy))\n",
    "accuracies.append(test_accuracy)\n",
    "end = time.time()\n",
    "times.append(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4bccd-7219-4968-b133-799b362e4e6b",
   "metadata": {},
   "source": [
    "If we increase the number of iterations to 400, the difference in the accuracy scores is only about 3.31% which is much closer. Additionally, the accuracies have not changed too greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5378649a-d639-4913-89c3-52b9e59a7911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "Total number of failed predictions: 564\n",
      "Percentage of failed predictions for training set: 47.87775891341256\n",
      "Percentage of class 1 prediction failure: 30.141843971631204\n",
      "Percentage of class 2 prediction failure: 33.156028368794324\n",
      "Percentage of class 3 prediction failure: 36.702127659574465\n",
      "TESTING SET\n",
      "Total number of failed predictions: 151\n",
      "Percentage of failed predictions for testing set: 51.186440677966104\n",
      "Percentage of class 1 prediction failure: 31.788079470198678\n",
      "Percentage of class 2 prediction failure: 37.74834437086093\n",
      "Percentage of class 3 prediction failure: 30.4635761589404\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "\n",
    "for i, j in zip(y_train,yhat_train):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TRAINING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for training set: \" + str(total/len(y_train)*100))\n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))\n",
    "\n",
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "for i, j in zip(y_test,yhat_test):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TESTING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for testing set: \" + str(total/len(y_test)*100))   \n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e17e47-9feb-4ebd-88a6-75a467967a54",
   "metadata": {},
   "source": [
    "##### 2.2.2.5 Analyzing Steepest Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023ebf6a-c842-432f-8f05-10637be14cd9",
   "metadata": {},
   "source": [
    "The highest accuracy scores we got from running steepest ascent on our training and testing data while maintaining a small difference between the accuracy scores was from applying only L2 regularization.\n",
    "\n",
    "The score for the training set was about 52.38%.\n",
    "\n",
    "The score for the testing set was about 50.85%.\n",
    "\n",
    "The difference in the accuracy scores was about 1.53%.\n",
    "\n",
    "The input parameters were:\n",
    "\n",
    "eta = 0.1\n",
    "\n",
    "iterations = 500\n",
    "\n",
    "C2 = 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba605e2-1111-4167-9b8b-c859aeddf8ac",
   "metadata": {},
   "source": [
    "Now, this accuracy is not very high. It is still only correctly predicting about 50% of the target values. We will move on to testing stochastic gradient ascent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34fcec-c4e6-4d4f-a1f0-ebd071fec955",
   "metadata": {},
   "source": [
    "#### 2.2.3 Using Stochastic Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d5a26-998e-410b-ae71-a214944d3e53",
   "metadata": {},
   "source": [
    "Next, we tested using stochastic gradient ascent as our optimization technique. This is the code we will run as we change the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ce9d4d6-ac1d-4227-9714-0621a109cd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[-0.18811791  0.07245339 -0.19021731 -0.07227116 -0.18554691  0.07405581\n",
      "   0.02150912  0.11178298 -0.11738238  0.09883417]\n",
      " [-0.53576971  0.07356847  0.1083039   0.03811106  0.09122598 -0.13288085\n",
      "  -0.00997808 -0.1234927   0.10127198  0.01579142]\n",
      " [-0.20258388 -0.20451139  0.02527662  0.05823889  0.07099961  0.05031841\n",
      "   0.05514439  0.0925428   0.0619625  -0.08254377]]\n",
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[-0.18811791  0.07245339 -0.19021731 -0.07227116 -0.18554691  0.07405581\n",
      "   0.02150912  0.11178298 -0.11738238  0.09883417]\n",
      " [-0.53576971  0.07356847  0.1083039   0.03811106  0.09122598 -0.13288085\n",
      "  -0.00997808 -0.1234927   0.10127198  0.01579142]\n",
      " [-0.20258388 -0.20451139  0.02527662  0.05823889  0.07099961  0.05031841\n",
      "   0.05514439  0.0925428   0.0619625  -0.08254377]]\n",
      "Accuracy: 49.23599320882852%\n",
      "CPU times: user 120 ms, sys: 425 ms, total: 545 ms\n",
      "Wall time: 37.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "slr = LogisticRegression(0.005, optimization_technique=\"stochastic gradient ascent\", iterations=500)\n",
    "slr.fit(X_train_scaled, y_train)\n",
    "print(slr)\n",
    "\n",
    "yhat = slr.predict(X_train_scaled)\n",
    "print(slr)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_train,yhat)*100) + \"%\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b402a09-0310-464b-b6c9-45d27f2cc66f",
   "metadata": {},
   "source": [
    "##### 2.2.3.1 Testing Different Learning Rates and Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e201d-fefa-4d82-9e79-0ee3d3ad49c8",
   "metadata": {},
   "source": [
    "We tested using the same set of learning rates as before while manually changing the iterations value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b25faee0-41a7-48b7-bc04-a4029036bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "etas = [0.001, 0.003, 0.005, 0.01, 0.03, 0.1, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf77d89b-c7d6-49e1-a0ed-f0d2cf8a714e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.001: 52.12224108658744%\n",
      "Accuracy of 0.003: 52.12224108658744%\n",
      "Accuracy of 0.005: 52.12224108658744%\n",
      "Accuracy of 0.01: 52.12224108658744%\n",
      "Accuracy of 0.03: 52.12224108658744%\n",
      "Accuracy of 0.1: 52.12224108658744%\n",
      "Accuracy of 0.3: 52.12224108658744%\n",
      "Best accuracy with eta=0.001: 52.12224108658744%\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "best_eta = 0\n",
    "\n",
    "iterations = 200\n",
    "\n",
    "for e in etas:\n",
    "    eta = e\n",
    "    slr = LogisticRegression(eta=eta, optimization_technique=\"stochastic gradient ascent\", iterations=iterations)\n",
    "    lr.fit(X_train_scaled,y_train)\n",
    "    yhat = lr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_eta = eta\n",
    "    print(\"Accuracy of \" + str(eta) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "    \n",
    "print(\"Best accuracy with eta=\" + str(best_eta) + \": \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9bd6d-5e77-406d-8b69-2eb1fd984b15",
   "metadata": {},
   "source": [
    "Running different eta values and iteration values did not change the accuracy score. Thus, we know that stochastic gradient ascent needs to apply a regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e04573-1cad-4547-bd72-04def1e67ca3",
   "metadata": {},
   "source": [
    "##### 2.2.3.2 Testing Applying L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b87cecba-d6f8-4b20-9bb1-24588991b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_vals = np.logspace(-3,1,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c858a842-515c-4341-a934-66fae2e5c379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.001: 49.40577249575552%\n",
      "Accuracy of 0.0019306977288832496: 49.830220713073004%\n",
      "Accuracy of 0.003727593720314938: 50.67911714770798%\n",
      "Accuracy of 0.0071968567300115215: 48.04753820033956%\n",
      "Accuracy of 0.013894954943731374: 51.44312393887945%\n",
      "Accuracy of 0.026826957952797246: 49.15110356536502%\n",
      "Accuracy of 0.0517947467923121: 50.169779286926996%\n",
      "Accuracy of 0.1: 50.93378607809848%\n",
      "Accuracy of 0.19306977288832497: 50.42444821731748%\n",
      "Accuracy of 0.3727593720314938: 48.302207130730054%\n",
      "Accuracy of 0.7196856730011514: 46.774193548387096%\n",
      "Accuracy of 1.3894954943731375: 46.6893039049236%\n",
      "Accuracy of 2.6826957952797246: 45.75551782682513%\n",
      "Accuracy of 5.179474679231207: 43.71816638370119%\n",
      "Accuracy of 10.0: 41.42614601018676%\n",
      "Best accuracy with C1=0.013894954943731374: 51.44312393887945%\n",
      "CPU times: user 1.57 s, sys: 5.25 s, total: 6.82 s\n",
      "Wall time: 435 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_accuracy = 0\n",
    "best_C1 = 0\n",
    "\n",
    "eta = 0.008\n",
    "iterations = 500\n",
    "for C in cost_vals:\n",
    "    C1 = C\n",
    "    slr = LogisticRegression(eta=eta, optimization_technique=\"stochastic gradient ascent\", iterations = iterations, regularization=\"L1\", C1=C1)\n",
    "    slr.fit(X_train_scaled, y_train)\n",
    "    yhat = slr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_C1 = C1\n",
    "    print(\"Accuracy of \" + str(C1) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "    \n",
    "print(\"Best accuracy with C1=\" + str(best_C1) + \": \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a04d0d-89cd-4ef2-86d6-5f43d8d0a6c6",
   "metadata": {},
   "source": [
    "Applying different cost values, learning rates, and iterations, our best accuracy score resulted in about 52.55%. \n",
    "\n",
    "The parameters were:\n",
    "\n",
    "eta = 0.008\n",
    "\n",
    "iterations = 500\n",
    "\n",
    "C = 0.026826957952797246."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13a3573a-2b88-4a21-98c6-b5ccfa99f940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C1=0.026826957952797246: 51.188455008488965%\n",
      "Accuracy of C1=0.026826957952797246: 49.152542372881356%\n",
      "2.035912635607609\n",
      "CPU times: user 96.1 ms, sys: 306 ms, total: 402 ms\n",
      "Wall time: 26.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eta = 0.008\n",
    "iterations = 500\n",
    "C1 = 0.026826957952797246\n",
    "\n",
    "slr = LogisticRegression(eta=eta, optimization_technique=\"stochastic gradient ascent\", iterations=iterations, regularization=\"L1\", C1=C1)\n",
    "slr.fit(X_train_scaled, y_train)\n",
    "yhat = slr.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train,yhat)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "\n",
    "yhat = slr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test,yhat)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \": \" + str(accuracy_score(y_test,yhat)*100) + \"%\")\n",
    "\n",
    "print(abs(train_accuracy-test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f06a2e-f118-4886-9c44-cb05f8f2b2ca",
   "metadata": {},
   "source": [
    "The difference in the accuracy scores was about 3.56%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c78d3af5-d65e-4246-a5c2-2e854bd512a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C1=0.026826957952797246: 49.9151103565365%\n",
      "Accuracy of C1=0.026826957952797246: 46.440677966101696%\n",
      "3.4744323904348065\n",
      "CPU times: user 153 ms, sys: 599 ms, total: 751 ms\n",
      "Wall time: 48.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eta = 0.008\n",
    "iterations = 450\n",
    "C1 = 0.026826957952797246\n",
    "\n",
    "slr = LogisticRegression(eta=eta, optimization_technique=\"stochastic gradient ascent\", iterations=iterations, regularization=\"L1\", C1=C1)\n",
    "slr.fit(X_train_scaled, y_train)\n",
    "yhat_train = slr.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train,yhat_train)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \": \" + str(accuracy_score(y_train,yhat_train)*100) + \"%\")\n",
    "\n",
    "yhat_test = slr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test,yhat_test)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \": \" + str(accuracy_score(y_test,yhat_test)*100) + \"%\")\n",
    "\n",
    "print(abs(train_accuracy-test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0dc3fc-1a36-45f9-aa9e-f10dfced3923",
   "metadata": {},
   "source": [
    "Changing the number of iterations gets our difference in accuracy scores lower. However, the accuracy of the training data decreased while the accuracy of the testing data increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "546a964c-497a-4417-8aaa-30ca1952775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "Total number of failed predictions: 590\n",
      "Percentage of failed predictions for training set: 50.08488964346349\n",
      "Percentage of class 1 prediction failure: 28.983050847457626\n",
      "Percentage of class 2 prediction failure: 30.508474576271187\n",
      "Percentage of class 3 prediction failure: 40.50847457627118\n",
      "TESTING SET\n",
      "Total number of failed predictions: 158\n",
      "Percentage of failed predictions for testing set: 53.559322033898304\n",
      "Percentage of class 1 prediction failure: 29.746835443037973\n",
      "Percentage of class 2 prediction failure: 36.075949367088604\n",
      "Percentage of class 3 prediction failure: 34.177215189873415\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "\n",
    "for i, j in zip(y_train,yhat_train):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TRAINING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for training set: \" + str(total/len(y_train)*100))\n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))\n",
    "\n",
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "for i, j in zip(y_test,yhat_test):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TESTING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for testing set: \" + str(total/len(y_test)*100))   \n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120a740-4618-4450-941e-cfbd8640d0c9",
   "metadata": {},
   "source": [
    "##### 2.2.3.3 Testing Applying L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cfbbe607-4106-47e5-9707-d76dadc01fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy0.001: 48.89643463497453%\n",
      "Accuracy0.0019306977288832496: 48.04753820033956%\n",
      "Accuracy0.003727593720314938: 47.53820033955857%\n",
      "Accuracy0.0071968567300115215: 50.33955857385399%\n",
      "Accuracy0.013894954943731374: 49.15110356536502%\n",
      "Accuracy0.026826957952797246: 49.06621392190153%\n",
      "Accuracy0.0517947467923121: 47.02886247877759%\n",
      "Accuracy0.1: 48.811544991511035%\n",
      "Accuracy0.19306977288832497: 52.03735144312394%\n",
      "Accuracy0.3727593720314938: 49.06621392190153%\n",
      "Accuracy0.7196856730011514: 42.02037351443124%\n",
      "Accuracy1.3894954943731375: 44.821731748726656%\n",
      "Accuracy2.6826957952797246: 41.25636672325976%\n",
      "Accuracy5.179474679231207: 41.76570458404075%\n",
      "Accuracy10.0: 39.473684210526315%\n",
      "Best accuracy with C2=0.19306977288832497: 52.03735144312394%\n",
      "CPU times: user 1.83 s, sys: 6.56 s, total: 8.39 s\n",
      "Wall time: 558 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_accuracy = 0\n",
    "best_C2 = 0\n",
    "\n",
    "eta = 0.005\n",
    "iterations = 500\n",
    "for C in cost_vals:\n",
    "    C2 = C\n",
    "    slr = LogisticRegression(eta=eta, optimization_technique=\"stochastic gradient ascent\", iterations = iterations, regularization=\"L2\", C2=C2)\n",
    "    slr.fit(X_train_scaled, y_train)\n",
    "    yhat = slr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_C2 = C2\n",
    "    print(\"Accuracy\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "    \n",
    "print(\"Best accuracy with C2=\" + str(best_C2) + \": \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2416a-9406-46bf-8c85-db07065b02c0",
   "metadata": {},
   "source": [
    "Applying different cost values, learning rates, and iterations, our best accuracy score resulted in about 51.19%. \n",
    "\n",
    "The parameters were:\n",
    "\n",
    "eta = 0.005\n",
    "\n",
    "iterations = 500\n",
    "\n",
    "C = 0.0517947467923121."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c5c964e-c296-41e1-bff3-796fee225996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C2=0.0517947467923121: 49.23599320882852%\n",
      "Accuracy of C2=0.0517947467923121: 46.779661016949156%\n",
      "2.456332191879362\n",
      "CPU times: user 133 ms, sys: 388 ms, total: 520 ms\n",
      "Wall time: 35.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eta = 0.005\n",
    "iterations = 500\n",
    "C2 = 0.0517947467923121\n",
    "\n",
    "slr = LogisticRegression(eta=eta, optimization_technique=\"stochastic gradient ascent\", iterations=iterations, regularization=\"L2\", C2=C2)\n",
    "slr.fit(X_train_scaled, y_train)\n",
    "yhat_train = slr.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train,yhat_train)*100\n",
    "print(\"Accuracy of C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat_train)*100) + \"%\")\n",
    "\n",
    "yhat_test = slr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test,yhat_test)*100\n",
    "print(\"Accuracy of C2=\" + str(C2) + \": \" + str(accuracy_score(y_test,yhat_test)*100) + \"%\")\n",
    "\n",
    "print(abs(train_accuracy-test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8bd14a-817c-4fd1-ac38-995a1921c11c",
   "metadata": {},
   "source": [
    "Everytime we run fit, our data will be fitted a little differently. Using the parameters as above, the difference in the accuracy scores was about 1.78%. The accuracy score of the training data decreased some."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa5cb2-dd3c-488e-bd1c-ce9b8c7a5479",
   "metadata": {},
   "source": [
    "Changing the parameters did not show any great increase in the accuracy scores or the difference in the accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57beab7b-22ba-4212-bccc-47b360ca42fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "Total number of failed predictions: 598\n",
      "Percentage of failed predictions for training set: 50.76400679117148\n",
      "Percentage of class 1 prediction failure: 35.618729096989966\n",
      "Percentage of class 2 prediction failure: 27.424749163879596\n",
      "Percentage of class 3 prediction failure: 36.95652173913043\n",
      "TESTING SET\n",
      "Total number of failed predictions: 157\n",
      "Percentage of failed predictions for testing set: 53.22033898305085\n",
      "Percentage of class 1 prediction failure: 37.57961783439491\n",
      "Percentage of class 2 prediction failure: 30.573248407643312\n",
      "Percentage of class 3 prediction failure: 31.84713375796178\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "\n",
    "for i, j in zip(y_train,yhat_train):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TRAINING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for training set: \" + str(total/len(y_train)*100))\n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))\n",
    "\n",
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "for i, j in zip(y_test,yhat_test):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TESTING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for testing set: \" + str(total/len(y_test)*100))   \n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db20c4-44fe-47b2-bd5e-8dcb4cc1ddb4",
   "metadata": {},
   "source": [
    "##### 2.2.3.4 Testing Applying Both L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07819dc9-c811-412c-9fa6-67259ff8bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_vals_combos = combinations_data(cost_vals, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3dacc48-491a-4a6c-8574-4e2bc0702b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C1=0.001 and C2=0.0019306977288832496: 49.49066213921902%\n",
      "Accuracy of C1=0.001 and C2=0.003727593720314938: 49.49066213921902%\n",
      "Accuracy of C1=0.001 and C2=0.0071968567300115215: 49.15110356536502%\n",
      "Accuracy of C1=0.001 and C2=0.013894954943731374: 49.23599320882852%\n",
      "Accuracy of C1=0.001 and C2=0.026826957952797246: 49.57555178268252%\n",
      "Accuracy of C1=0.001 and C2=0.0517947467923121: 48.811544991511035%\n",
      "Accuracy of C1=0.001 and C2=0.1: 48.72665534804754%\n",
      "Accuracy of C1=0.001 and C2=0.19306977288832497: 50.25466893039049%\n",
      "Accuracy of C1=0.001 and C2=0.3727593720314938: 50.169779286926996%\n",
      "Accuracy of C1=0.001 and C2=0.7196856730011514: 49.06621392190153%\n",
      "Accuracy of C1=0.001 and C2=1.3894954943731375: 43.803056027164686%\n",
      "Accuracy of C1=0.001 and C2=2.6826957952797246: 46.774193548387096%\n",
      "Accuracy of C1=0.001 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.001 and C2=10.0: 42.52971137521222%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.003727593720314938: 49.49066213921902%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.0071968567300115215: 49.660441426146015%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.013894954943731374: 49.49066213921902%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.026826957952797246: 48.38709677419355%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.0517947467923121: 48.47198641765705%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.1: 47.283531409168084%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.19306977288832497: 47.87775891341256%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.3727593720314938: 46.5195246179966%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.7196856730011514: 48.04753820033956%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=1.3894954943731375: 42.52971137521222%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=2.6826957952797246: 47.792869269949065%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=10.0: 42.52971137521222%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.0071968567300115215: 47.792869269949065%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.013894954943731374: 48.55687606112054%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.026826957952797246: 48.217317487266556%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.0517947467923121: 48.64176570458404%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.1: 50.59422750424448%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.19306977288832497: 50.50933786078099%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.3727593720314938: 46.94397283531409%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.7196856730011514: 49.06621392190153%\n",
      "Accuracy of C1=0.003727593720314938 and C2=1.3894954943731375: 46.6044142614601%\n",
      "Accuracy of C1=0.003727593720314938 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.003727593720314938 and C2=5.179474679231207: 41.42614601018676%\n",
      "Accuracy of C1=0.003727593720314938 and C2=10.0: 43.12393887945671%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.013894954943731374: 48.04753820033956%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.026826957952797246: 48.302207130730054%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.0517947467923121: 51.103565365025474%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.1: 48.13242784380306%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.19306977288832497: 48.98132427843803%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.3727593720314938: 48.55687606112054%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.7196856730011514: 47.70797962648557%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=1.3894954943731375: 44.821731748726656%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=5.179474679231207: 45.16129032258064%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=10.0: 41.76570458404075%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.026826957952797246: 49.49066213921902%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.0517947467923121: 46.94397283531409%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.1: 48.89643463497453%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.19306977288832497: 48.04753820033956%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.3727593720314938: 51.61290322580645%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.7196856730011514: 48.98132427843803%\n",
      "Accuracy of C1=0.013894954943731374 and C2=1.3894954943731375: 47.283531409168084%\n",
      "Accuracy of C1=0.013894954943731374 and C2=2.6826957952797246: 42.78438030560272%\n",
      "Accuracy of C1=0.013894954943731374 and C2=5.179474679231207: 41.68081494057725%\n",
      "Accuracy of C1=0.013894954943731374 and C2=10.0: 41.76570458404075%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.0517947467923121: 49.15110356536502%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.1: 47.45331069609507%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.19306977288832497: 48.98132427843803%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.3727593720314938: 48.98132427843803%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.7196856730011514: 47.198641765704586%\n",
      "Accuracy of C1=0.026826957952797246 and C2=1.3894954943731375: 41.935483870967744%\n",
      "Accuracy of C1=0.026826957952797246 and C2=2.6826957952797246: 43.378607809847196%\n",
      "Accuracy of C1=0.026826957952797246 and C2=5.179474679231207: 42.10526315789473%\n",
      "Accuracy of C1=0.026826957952797246 and C2=10.0: 41.76570458404075%\n",
      "Accuracy of C1=0.0517947467923121 and C2=0.1: 47.283531409168084%\n",
      "Accuracy of C1=0.0517947467923121 and C2=0.19306977288832497: 46.5195246179966%\n",
      "Accuracy of C1=0.0517947467923121 and C2=0.3727593720314938: 47.53820033955857%\n",
      "Accuracy of C1=0.0517947467923121 and C2=0.7196856730011514: 44.65195246179966%\n",
      "Accuracy of C1=0.0517947467923121 and C2=1.3894954943731375: 45.50084889643464%\n",
      "Accuracy of C1=0.0517947467923121 and C2=2.6826957952797246: 41.850594227504246%\n",
      "Accuracy of C1=0.0517947467923121 and C2=5.179474679231207: 43.378607809847196%\n",
      "Accuracy of C1=0.0517947467923121 and C2=10.0: 46.09507640067912%\n",
      "Accuracy of C1=0.1 and C2=0.19306977288832497: 48.13242784380306%\n",
      "Accuracy of C1=0.1 and C2=0.3727593720314938: 47.62308998302207%\n",
      "Accuracy of C1=0.1 and C2=0.7196856730011514: 43.463497453310694%\n",
      "Accuracy of C1=0.1 and C2=1.3894954943731375: 43.463497453310694%\n",
      "Accuracy of C1=0.1 and C2=2.6826957952797246: 45.840407470288625%\n",
      "Accuracy of C1=0.1 and C2=5.179474679231207: 41.68081494057725%\n",
      "Accuracy of C1=0.1 and C2=10.0: 41.76570458404075%\n",
      "Accuracy of C1=0.19306977288832497 and C2=0.3727593720314938: 47.62308998302207%\n",
      "Accuracy of C1=0.19306977288832497 and C2=0.7196856730011514: 48.64176570458404%\n",
      "Accuracy of C1=0.19306977288832497 and C2=1.3894954943731375: 41.17147707979626%\n",
      "Accuracy of C1=0.19306977288832497 and C2=2.6826957952797246: 41.76570458404075%\n",
      "Accuracy of C1=0.19306977288832497 and C2=5.179474679231207: 43.97283531409168%\n",
      "Accuracy of C1=0.19306977288832497 and C2=10.0: 41.76570458404075%\n",
      "Accuracy of C1=0.3727593720314938 and C2=0.7196856730011514: 44.821731748726656%\n",
      "Accuracy of C1=0.3727593720314938 and C2=1.3894954943731375: 46.5195246179966%\n",
      "Accuracy of C1=0.3727593720314938 and C2=2.6826957952797246: 43.54838709677419%\n",
      "Accuracy of C1=0.3727593720314938 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=0.3727593720314938 and C2=10.0: 41.76570458404075%\n",
      "Accuracy of C1=0.7196856730011514 and C2=1.3894954943731375: 44.48217317487266%\n",
      "Accuracy of C1=0.7196856730011514 and C2=2.6826957952797246: 41.68081494057725%\n",
      "Accuracy of C1=0.7196856730011514 and C2=5.179474679231207: 42.10526315789473%\n",
      "Accuracy of C1=0.7196856730011514 and C2=10.0: 45.331069609507644%\n",
      "Accuracy of C1=1.3894954943731375 and C2=2.6826957952797246: 44.821731748726656%\n",
      "Accuracy of C1=1.3894954943731375 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=1.3894954943731375 and C2=10.0: 41.76570458404075%\n",
      "Accuracy of C1=2.6826957952797246 and C2=5.179474679231207: 41.76570458404075%\n",
      "Accuracy of C1=2.6826957952797246 and C2=10.0: 41.76570458404075%\n",
      "Accuracy of C1=5.179474679231207 and C2=10.0: 41.76570458404075%\n",
      "Best accuracy with C1=0.013894954943731374 and C2=0.3727593720314938: 51.61290322580645%\n",
      "CPU times: user 11.3 s, sys: 41.8 s, total: 53.1 s\n",
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "iterations = 400\n",
    "eta = 0.005\n",
    "\n",
    "best_accuracy = 0\n",
    "best_C1 = 0\n",
    "best_C2 = 0\n",
    "\n",
    "for i in cost_vals_combos:\n",
    "    C1 = i[0]\n",
    "    C2 = i[1]\n",
    "    slr = LogisticRegression(eta=eta, optimization_technique=\"stochastic gradient ascent\", iterations = iterations, regularization=\"both\", C1=C1, C2=C2)\n",
    "    slr.fit(X_train_scaled, y_train)\n",
    "    yhat = slr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_C1 = C1\n",
    "        best_C2 = C2\n",
    "    print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "\n",
    "for i in cost_vals_combos:\n",
    "    C1 = i[1]\n",
    "    C2 = i[0]\n",
    "    slr = LogisticRegression(eta=eta, optimization_technique=\"stochastic gradient ascent\", iterations = iterations, regularization=\"both\", C1=C1, C2=C2)\n",
    "    slr.fit(X_train_scaled, y_train)\n",
    "    yhat = slr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_C1 = C1\n",
    "        best_C2 = C2\n",
    "    print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "    \n",
    "print(\"Best accuracy with C1=\" + str(best_C1) + \" and C2=\" + str(best_C2) + \": \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac445d-6281-4387-8482-6f6808b8ea05",
   "metadata": {},
   "source": [
    "The best accuracy was about 51.53%. \n",
    "\n",
    "The parameters were:\n",
    "\n",
    "eta = 0.005\n",
    "\n",
    "iterations = 400\n",
    "\n",
    "C1 = 0.013894954943731374\n",
    "\n",
    "C2 = 0.026826957952797246."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f81fbcd5-7479-41f1-8cd2-217c82de299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C1=0.013894954943731374 and C2=0.026826957952797246: 48.04753820033956%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.026826957952797246: 46.779661016949156%\n",
      "1.2678771833904037\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "eta = 0.005\n",
    "iterations = 400\n",
    "C1 = 0.013894954943731374\n",
    "C2 = 0.026826957952797246\n",
    "\n",
    "slr = LogisticRegression(eta=eta, optimization_technique=\"stochastic gradient ascent\", iterations = iterations, regularization=\"both\", C1=C1, C2=C2)\n",
    "slr.fit(X_train_scaled, y_train)\n",
    "yhat_train = slr.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train,yhat_train)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat_train)*100) + \"%\")\n",
    "\n",
    "yhat_test = slr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test,yhat_test)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_test,yhat_test)*100) + \"%\")\n",
    "print(abs(train_accuracy-test_accuracy))\n",
    "accuracies.append(test_accuracy)\n",
    "end = time.time()\n",
    "times.append(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39419985-b867-4209-96aa-fc78dfc0a736",
   "metadata": {},
   "source": [
    "Running fit multiple times changed the outcomes. However, the range of difference in accuracy scores was roughly between 1% and 5%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b1f21338-0298-4961-98b9-43b36dbc160d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "Total number of failed predictions: 612\n",
      "Percentage of failed predictions for training set: 51.95246179966044\n",
      "Percentage of class 1 prediction failure: 31.209150326797385\n",
      "Percentage of class 2 prediction failure: 17.15686274509804\n",
      "Percentage of class 3 prediction failure: 51.633986928104584\n",
      "TESTING SET\n",
      "Total number of failed predictions: 157\n",
      "Percentage of failed predictions for testing set: 53.22033898305085\n",
      "Percentage of class 1 prediction failure: 38.853503184713375\n",
      "Percentage of class 2 prediction failure: 21.019108280254777\n",
      "Percentage of class 3 prediction failure: 40.12738853503185\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "\n",
    "for i, j in zip(y_train,yhat_train):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TRAINING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for training set: \" + str(total/len(y_train)*100))\n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))\n",
    "\n",
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "for i, j in zip(y_test,yhat_test):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TESTING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for testing set: \" + str(total/len(y_test)*100))   \n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86855caf-74a3-48aa-b444-b1f640ac3a62",
   "metadata": {},
   "source": [
    "##### 2.2.3.5 Analyzing Stochastic Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb3cbd-ff5d-4ebd-9a75-5a3c461d6ab5",
   "metadata": {},
   "source": [
    "The accuracy scores using the stochastic gradient ascent while applying different regularizations did not exceed the accuracy scores from the previous section of testing steepest ascent technique.\n",
    "\n",
    "The highest accuracy scores with smallest amount of difference between those of the training and testing set resulted from applying L1 regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc5e48-e5d2-417c-bb23-3bf8c46e56e2",
   "metadata": {},
   "source": [
    "#### 2.2.4 Using Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb8ba83-db2b-43ab-a221-6ceba82155a5",
   "metadata": {},
   "source": [
    "Lastly, we tested using Newton's Method as our optimization technique. This is the code we will run as we change the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "104846c5-7b1b-4097-ae95-826ef9d49d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[-0.35646086  0.64770674 -0.50225679 -0.05900932 -0.80202128  0.19078423\n",
      "  -0.0306416  -0.06060872 -0.26250435  0.13469809]\n",
      " [-1.49691364  0.11163209  0.67804647 -0.04923907  0.3552094  -0.15557073\n",
      "  -0.09611857 -0.13831939  0.19024755 -0.0444875 ]\n",
      " [-0.64536642 -0.81057042  0.07734759  0.09781604  0.58270551 -0.07184315\n",
      "   0.09330705  0.16862562  0.14913696 -0.13303049]]\n",
      "Accuracy: 52.03735144312394%\n",
      "CPU times: user 155 ms, sys: 660 ms, total: 816 ms\n",
      "Wall time: 54.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hlr = LogisticRegression(eta=1.0, optimization_technique=\"Newton's method\", iterations=4, C1=0.001)\n",
    "\n",
    "hlr.fit(X_train_scaled,y_train)\n",
    "print(hlr)\n",
    "\n",
    "yhat = hlr.predict(X_train_scaled)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_train,yhat)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e57cb-537d-473d-90ad-d9ec0eb36c5c",
   "metadata": {},
   "source": [
    "Since we're adding in the regularization term in our code, we will forgo running with only varying learning rates and iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da21621-99ef-4ed1-99f6-f52b4607842d",
   "metadata": {},
   "source": [
    "##### 2.2.4.1 Testing Applying L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22f7dd3d-2412-4e92-92e1-ea00cb310212",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_vals = np.logspace(-3,1,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "869fd05b-bad0-4715-a02f-c1c625ca35cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.001: 51.782682512733444%\n",
      "Accuracy of 0.0019306977288832496: 51.782682512733444%\n",
      "Accuracy of 0.003727593720314938: 51.782682512733444%\n",
      "Accuracy of 0.0071968567300115215: 51.782682512733444%\n",
      "Accuracy of 0.013894954943731374: 51.782682512733444%\n",
      "Accuracy of 0.026826957952797246: 51.782682512733444%\n",
      "Accuracy of 0.0517947467923121: 51.782682512733444%\n",
      "Accuracy of 0.1: 51.782682512733444%\n",
      "Accuracy of 0.19306977288832497: 51.782682512733444%\n",
      "Accuracy of 0.3727593720314938: 52.03735144312394%\n",
      "Accuracy of 0.7196856730011514: 52.03735144312394%\n",
      "Accuracy of 1.3894954943731375: 52.03735144312394%\n",
      "Accuracy of 2.6826957952797246: 52.207130730050935%\n",
      "Accuracy of 5.179474679231207: 53.140916808149406%\n",
      "Accuracy of 10.0: 46.17996604414261%\n",
      "Best accuracy with C1=5.179474679231207: 53.140916808149406%\n",
      "CPU times: user 2.38 s, sys: 3.52 s, total: 5.89 s\n",
      "Wall time: 385 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_accuracy = 0\n",
    "best_C1 = 0\n",
    "\n",
    "eta = 0.001\n",
    "iterations = 5\n",
    "for C in cost_vals:\n",
    "    C1 = C\n",
    "    hlr = LogisticRegression(eta=eta, optimization_technique=\"Newton's method\", iterations = iterations, regularization=\"L1\", C1=C1)\n",
    "    hlr.fit(X_train_scaled, y_train)\n",
    "    yhat = hlr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_C1 = C1\n",
    "    print(\"Accuracy of \" + str(C1) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "    \n",
    "print(\"Best accuracy with C1=\" + str(best_C1) + \": \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babc116-d480-4f06-98b6-c5eee3acd504",
   "metadata": {},
   "source": [
    "The best accuracy we got for our training set was with these parameters:\n",
    "\n",
    "eta = 0.001\n",
    "\n",
    "iterations = 5\n",
    "\n",
    "C1 = 5.179474679231207.\n",
    "\n",
    "The accuracy score for our training set was about 53.14%. We were getting slightly larger accuracy scores on average than using the previous two techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a9a7a907-79ae-462a-8866-6526cd280d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C1=5.179474679231207: 53.140916808149406%\n",
      "Accuracy of C1=5.179474679231207: 49.49152542372882%\n",
      "3.649391384420589\n",
      "CPU times: user 181 ms, sys: 525 ms, total: 706 ms\n",
      "Wall time: 46.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eta = 0.001\n",
    "iterations = 5\n",
    "C1 = 5.179474679231207\n",
    "\n",
    "hlr = LogisticRegression(eta=eta, optimization_technique=\"Newton's method\", iterations=iterations, regularization=\"L1\", C1=C1)\n",
    "hlr.fit(X_train_scaled, y_train)\n",
    "yhat = hlr.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train,yhat)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "\n",
    "yhat = hlr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test,yhat)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \": \" + str(accuracy_score(y_test,yhat)*100) + \"%\")\n",
    "\n",
    "print(abs(train_accuracy-test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7774c-ca7a-4c46-bca7-01592712f198",
   "metadata": {},
   "source": [
    "The difference in our accuracy scores using these parameters was about 3.65%. Both accuracy scores were fairly high compared to other ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6423dc02-a43d-4c75-92c9-0bddb5b95572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C1=0.001: 52.29202037351443%\n",
      "Accuracy of C1=0.001: 50.16949152542372%\n",
      "2.1225288480907096\n",
      "CPU times: user 182 ms, sys: 231 ms, total: 412 ms\n",
      "Wall time: 27 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eta = 1\n",
    "iterations = 2\n",
    "C1 = 0.001\n",
    "\n",
    "hlr = LogisticRegression(eta=eta, optimization_technique=\"Newton's method\", iterations=iterations, regularization=\"L1\", C1=C1)\n",
    "hlr.fit(X_train_scaled, y_train)\n",
    "yhat_train = hlr.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train,yhat_train)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \": \" + str(accuracy_score(y_train,yhat_train)*100) + \"%\")\n",
    "\n",
    "yhat_test = hlr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test,yhat_test)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \": \" + str(accuracy_score(y_test,yhat_test)*100) + \"%\")\n",
    "\n",
    "print(abs(train_accuracy-test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebefc78-152a-4e92-a5af-6960b0de8ddc",
   "metadata": {},
   "source": [
    "Changing the values of the parameters did help decrease the difference in the accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4417c9f6-ea8b-4d65-87d1-506ef8de6e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "Total number of failed predictions: 562\n",
      "Percentage of failed predictions for training set: 47.70797962648557\n",
      "Percentage of class 1 prediction failure: 32.74021352313167\n",
      "Percentage of class 2 prediction failure: 31.672597864768683\n",
      "Percentage of class 3 prediction failure: 35.587188612099645\n",
      "TESTING SET\n",
      "Total number of failed predictions: 147\n",
      "Percentage of failed predictions for testing set: 49.83050847457628\n",
      "Percentage of class 1 prediction failure: 31.292517006802722\n",
      "Percentage of class 2 prediction failure: 38.775510204081634\n",
      "Percentage of class 3 prediction failure: 29.931972789115648\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "\n",
    "for i, j in zip(y_train,yhat_train):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TRAINING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for training set: \" + str(total/len(y_train)*100))\n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))\n",
    "\n",
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "for i, j in zip(y_test,yhat_test):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TESTING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for testing set: \" + str(total/len(y_test)*100))   \n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a8afd-171e-462e-89d8-47580363e1f8",
   "metadata": {},
   "source": [
    "##### 2.2.4.2 Testing Applying L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f17eae39-ece6-452e-bd7c-efc4ecd7c8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.001: 52.03735144312394%\n",
      "Accuracy of 0.0019306977288832496: 52.03735144312394%\n",
      "Accuracy of 0.003727593720314938: 52.03735144312394%\n",
      "Accuracy of 0.0071968567300115215: 52.03735144312394%\n",
      "Accuracy of 0.013894954943731374: 52.03735144312394%\n",
      "Accuracy of 0.026826957952797246: 52.03735144312394%\n",
      "Accuracy of 0.0517947467923121: 52.03735144312394%\n",
      "Accuracy of 0.1: 52.03735144312394%\n",
      "Accuracy of 0.19306977288832497: 52.03735144312394%\n",
      "Accuracy of 0.3727593720314938: 52.03735144312394%\n",
      "Accuracy of 0.7196856730011514: 52.03735144312394%\n",
      "Accuracy of 1.3894954943731375: 51.95246179966044%\n",
      "Accuracy of 2.6826957952797246: 52.207130730050935%\n",
      "Accuracy of 5.179474679231207: 52.46179966044142%\n",
      "Accuracy of 10.0: 52.207130730050935%\n",
      "Best accuracy with C1=5.179474679231207: 52.46179966044142%\n",
      "CPU times: user 2.2 s, sys: 3.57 s, total: 5.77 s\n",
      "Wall time: 379 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_accuracy = 0\n",
    "best_C2 = 0\n",
    "\n",
    "eta = 1\n",
    "iterations = 4\n",
    "for C in cost_vals:\n",
    "    C2 = C\n",
    "    hlr = LogisticRegression(eta=eta, optimization_technique=\"Newton's method\", iterations = iterations, regularization=\"L2\", C2=C2)\n",
    "    hlr.fit(X_train_scaled, y_train)\n",
    "    yhat = hlr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_C2 = C2\n",
    "    print(\"Accuracy of \" + str(C2) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "    \n",
    "print(\"Best accuracy with C1=\" + str(best_C2) + \": \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f9967-adc3-47a2-b0ac-125b4fe7c242",
   "metadata": {},
   "source": [
    "The best accuracy score for applying L2 regularization had these parameters:\n",
    "\n",
    "eta = 1\n",
    "\n",
    "iterations = 4\n",
    "\n",
    "C2 = 5.179474679231207.\n",
    "\n",
    "The resulting accuracy score was about 52.46%. This is fairly similar to that of applying L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "993d1f30-0f21-4ad9-b5df-a9525b1a9c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C2=5.179474679231207: 52.46179966044142%\n",
      "Accuracy of C2=5.179474679231207: 50.50847457627119%\n",
      "1.9533250841702312\n",
      "CPU times: user 268 ms, sys: 368 ms, total: 636 ms\n",
      "Wall time: 42.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eta = 1\n",
    "iterations = 4\n",
    "C2 = 5.179474679231207\n",
    "\n",
    "hlr = LogisticRegression(eta=eta, optimization_technique=\"Newton's method\", iterations=iterations, regularization=\"L2\", C2=C2)\n",
    "hlr.fit(X_train_scaled, y_train)\n",
    "yhat = hlr.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train,yhat)*100\n",
    "print(\"Accuracy of C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "\n",
    "yhat = hlr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test,yhat)*100\n",
    "print(\"Accuracy of C2=\" + str(C2) + \": \" + str(accuracy_score(y_test,yhat)*100) + \"%\")\n",
    "\n",
    "print(abs(train_accuracy-test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9726f070-01a0-465c-958c-3a05e1b5bf79",
   "metadata": {},
   "source": [
    "The resulting difference in accuracy scores of our training and testing sets was about 1.95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b96cbb3-8db7-41fd-bba9-57a7fa350c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C2=0.005: 51.782682512733444%\n",
      "Accuracy of C2=0.005: 48.8135593220339%\n",
      "2.9691231906995412\n",
      "CPU times: user 19.5 ms, sys: 102 ms, total: 122 ms\n",
      "Wall time: 7.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eta = 0.005\n",
    "iterations = 1\n",
    "C2 = 0.005\n",
    "\n",
    "hlr = LogisticRegression(eta=eta, optimization_technique=\"Newton's method\", iterations=iterations, regularization=\"L2\", C2=C2)\n",
    "hlr.fit(X_train_scaled, y_train)\n",
    "yhat_train = hlr.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train,yhat_train)*100\n",
    "print(\"Accuracy of C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat_train)*100) + \"%\")\n",
    "\n",
    "yhat_test = hlr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test,yhat_test)*100\n",
    "print(\"Accuracy of C2=\" + str(C2) + \": \" + str(accuracy_score(y_test,yhat_test)*100) + \"%\")\n",
    "\n",
    "print(abs(train_accuracy-test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4675b78b-a93b-46c0-9350-4386a1b82740",
   "metadata": {},
   "source": [
    "Changing the values of the parameters did not decrease the difference in accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a2861d31-fb99-4eb9-9abf-a1aa53a8519a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "Total number of failed predictions: 568\n",
      "Percentage of failed predictions for training set: 48.217317487266556\n",
      "Percentage of class 1 prediction failure: 32.3943661971831\n",
      "Percentage of class 2 prediction failure: 33.978873239436616\n",
      "Percentage of class 3 prediction failure: 33.62676056338028\n",
      "TESTING SET\n",
      "Total number of failed predictions: 151\n",
      "Percentage of failed predictions for testing set: 51.186440677966104\n",
      "Percentage of class 1 prediction failure: 31.788079470198678\n",
      "Percentage of class 2 prediction failure: 39.735099337748345\n",
      "Percentage of class 3 prediction failure: 28.47682119205298\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "\n",
    "for i, j in zip(y_train,yhat_train):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TRAINING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for training set: \" + str(total/len(y_train)*100))\n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))\n",
    "\n",
    "total = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "for i, j in zip(y_test,yhat_test):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "\n",
    "print(\"TESTING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for testing set: \" + str(total/len(y_test)*100))   \n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e8209-1115-4526-a7dd-a7b76f83933c",
   "metadata": {},
   "source": [
    "##### 2.2.4.3 Testing Applying Both L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "97d18970-7fdc-4f4b-8a15-5b8820738862",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_vals_combos = combinations_data(cost_vals, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6bd9ed47-4888-4374-aa70-212083351844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C1=0.001 and C2=0.0019306977288832496: 52.03735144312394%\n",
      "Accuracy of C1=0.001 and C2=0.003727593720314938: 52.03735144312394%\n",
      "Accuracy of C1=0.001 and C2=0.0071968567300115215: 52.03735144312394%\n",
      "Accuracy of C1=0.001 and C2=0.013894954943731374: 52.03735144312394%\n",
      "Accuracy of C1=0.001 and C2=0.026826957952797246: 52.03735144312394%\n",
      "Accuracy of C1=0.001 and C2=0.0517947467923121: 52.03735144312394%\n",
      "Accuracy of C1=0.001 and C2=0.1: 52.03735144312394%\n",
      "Accuracy of C1=0.001 and C2=0.19306977288832497: 52.03735144312394%\n",
      "Accuracy of C1=0.001 and C2=0.3727593720314938: 52.03735144312394%\n",
      "Accuracy of C1=0.001 and C2=0.7196856730011514: 52.03735144312394%\n",
      "Accuracy of C1=0.001 and C2=1.3894954943731375: 51.95246179966044%\n",
      "Accuracy of C1=0.001 and C2=2.6826957952797246: 52.207130730050935%\n",
      "Accuracy of C1=0.001 and C2=5.179474679231207: 52.46179966044142%\n",
      "Accuracy of C1=0.001 and C2=10.0: 52.207130730050935%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.003727593720314938: 52.03735144312394%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.0071968567300115215: 52.03735144312394%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.013894954943731374: 52.03735144312394%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.026826957952797246: 52.03735144312394%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.0517947467923121: 52.03735144312394%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.1: 52.03735144312394%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.19306977288832497: 52.03735144312394%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.3727593720314938: 52.03735144312394%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=0.7196856730011514: 52.03735144312394%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=1.3894954943731375: 51.95246179966044%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=2.6826957952797246: 52.207130730050935%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=5.179474679231207: 52.46179966044142%\n",
      "Accuracy of C1=0.0019306977288832496 and C2=10.0: 52.207130730050935%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.0071968567300115215: 52.03735144312394%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.013894954943731374: 52.03735144312394%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.026826957952797246: 52.03735144312394%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.0517947467923121: 52.03735144312394%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.1: 52.03735144312394%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.19306977288832497: 52.03735144312394%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.3727593720314938: 52.03735144312394%\n",
      "Accuracy of C1=0.003727593720314938 and C2=0.7196856730011514: 52.03735144312394%\n",
      "Accuracy of C1=0.003727593720314938 and C2=1.3894954943731375: 51.95246179966044%\n",
      "Accuracy of C1=0.003727593720314938 and C2=2.6826957952797246: 52.207130730050935%\n",
      "Accuracy of C1=0.003727593720314938 and C2=5.179474679231207: 52.46179966044142%\n",
      "Accuracy of C1=0.003727593720314938 and C2=10.0: 52.207130730050935%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.013894954943731374: 52.03735144312394%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.026826957952797246: 52.03735144312394%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.0517947467923121: 52.03735144312394%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.1: 52.03735144312394%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.19306977288832497: 52.03735144312394%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.3727593720314938: 52.03735144312394%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=0.7196856730011514: 52.03735144312394%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=1.3894954943731375: 51.95246179966044%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=2.6826957952797246: 52.207130730050935%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=5.179474679231207: 52.46179966044142%\n",
      "Accuracy of C1=0.0071968567300115215 and C2=10.0: 52.207130730050935%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.026826957952797246: 52.03735144312394%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.0517947467923121: 52.03735144312394%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.1: 52.03735144312394%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.19306977288832497: 52.03735144312394%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.3727593720314938: 52.03735144312394%\n",
      "Accuracy of C1=0.013894954943731374 and C2=0.7196856730011514: 52.03735144312394%\n",
      "Accuracy of C1=0.013894954943731374 and C2=1.3894954943731375: 51.95246179966044%\n",
      "Accuracy of C1=0.013894954943731374 and C2=2.6826957952797246: 52.207130730050935%\n",
      "Accuracy of C1=0.013894954943731374 and C2=5.179474679231207: 52.46179966044142%\n",
      "Accuracy of C1=0.013894954943731374 and C2=10.0: 52.207130730050935%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.0517947467923121: 52.03735144312394%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.1: 52.03735144312394%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.19306977288832497: 52.03735144312394%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.3727593720314938: 52.03735144312394%\n",
      "Accuracy of C1=0.026826957952797246 and C2=0.7196856730011514: 52.03735144312394%\n",
      "Accuracy of C1=0.026826957952797246 and C2=1.3894954943731375: 51.95246179966044%\n",
      "Accuracy of C1=0.026826957952797246 and C2=2.6826957952797246: 52.207130730050935%\n",
      "Accuracy of C1=0.026826957952797246 and C2=5.179474679231207: 52.46179966044142%\n",
      "Accuracy of C1=0.026826957952797246 and C2=10.0: 52.207130730050935%\n",
      "Accuracy of C1=0.0517947467923121 and C2=0.1: 52.03735144312394%\n",
      "Accuracy of C1=0.0517947467923121 and C2=0.19306977288832497: 52.03735144312394%\n",
      "Accuracy of C1=0.0517947467923121 and C2=0.3727593720314938: 52.03735144312394%\n",
      "Accuracy of C1=0.0517947467923121 and C2=0.7196856730011514: 52.03735144312394%\n",
      "Accuracy of C1=0.0517947467923121 and C2=1.3894954943731375: 51.95246179966044%\n",
      "Accuracy of C1=0.0517947467923121 and C2=2.6826957952797246: 52.207130730050935%\n",
      "Accuracy of C1=0.0517947467923121 and C2=5.179474679231207: 52.46179966044142%\n",
      "Accuracy of C1=0.0517947467923121 and C2=10.0: 52.207130730050935%\n",
      "Accuracy of C1=0.1 and C2=0.19306977288832497: 52.03735144312394%\n",
      "Accuracy of C1=0.1 and C2=0.3727593720314938: 52.03735144312394%\n",
      "Accuracy of C1=0.1 and C2=0.7196856730011514: 52.03735144312394%\n",
      "Accuracy of C1=0.1 and C2=1.3894954943731375: 51.95246179966044%\n",
      "Accuracy of C1=0.1 and C2=2.6826957952797246: 52.207130730050935%\n",
      "Accuracy of C1=0.1 and C2=5.179474679231207: 52.46179966044142%\n",
      "Accuracy of C1=0.1 and C2=10.0: 52.207130730050935%\n",
      "Accuracy of C1=0.19306977288832497 and C2=0.3727593720314938: 52.03735144312394%\n",
      "Accuracy of C1=0.19306977288832497 and C2=0.7196856730011514: 52.03735144312394%\n",
      "Accuracy of C1=0.19306977288832497 and C2=1.3894954943731375: 51.95246179966044%\n",
      "Accuracy of C1=0.19306977288832497 and C2=2.6826957952797246: 52.207130730050935%\n",
      "Accuracy of C1=0.19306977288832497 and C2=5.179474679231207: 52.46179966044142%\n",
      "Accuracy of C1=0.19306977288832497 and C2=10.0: 52.207130730050935%\n",
      "Accuracy of C1=0.3727593720314938 and C2=0.7196856730011514: 51.86757215619694%\n",
      "Accuracy of C1=0.3727593720314938 and C2=1.3894954943731375: 51.95246179966044%\n",
      "Accuracy of C1=0.3727593720314938 and C2=2.6826957952797246: 52.207130730050935%\n",
      "Accuracy of C1=0.3727593720314938 and C2=5.179474679231207: 52.46179966044142%\n",
      "Accuracy of C1=0.3727593720314938 and C2=10.0: 52.207130730050935%\n",
      "Accuracy of C1=0.7196856730011514 and C2=1.3894954943731375: 52.03735144312394%\n",
      "Accuracy of C1=0.7196856730011514 and C2=2.6826957952797246: 52.207130730050935%\n",
      "Accuracy of C1=0.7196856730011514 and C2=5.179474679231207: 52.54668930390493%\n",
      "Accuracy of C1=0.7196856730011514 and C2=10.0: 52.207130730050935%\n",
      "Accuracy of C1=1.3894954943731375 and C2=2.6826957952797246: 52.29202037351443%\n",
      "Accuracy of C1=1.3894954943731375 and C2=5.179474679231207: 52.46179966044142%\n",
      "Accuracy of C1=1.3894954943731375 and C2=10.0: 52.12224108658744%\n",
      "Accuracy of C1=2.6826957952797246 and C2=5.179474679231207: 52.29202037351443%\n",
      "Accuracy of C1=2.6826957952797246 and C2=10.0: 51.86757215619694%\n",
      "Accuracy of C1=5.179474679231207 and C2=10.0: 44.05772495755518%\n",
      "Best accuracy with C1=0.7196856730011514 and C2=5.179474679231207: 52.54668930390493%\n",
      "CPU times: user 15.8 s, sys: 20.6 s, total: 36.4 s\n",
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "iterations = 5\n",
    "eta = 1\n",
    "\n",
    "best_accuracy = 0\n",
    "best_C1 = 0\n",
    "best_C2 = 0\n",
    "\n",
    "for i in cost_vals_combos:\n",
    "    C1 = i[0]\n",
    "    C2 = i[1]\n",
    "    hlr = LogisticRegression(eta=eta, optimization_technique=\"Newton's method\", iterations = iterations, regularization=\"both\", C1=C1, C2=C2)\n",
    "    hlr.fit(X_train_scaled, y_train)\n",
    "    yhat = hlr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_C1 = C1\n",
    "        best_C2 = C2\n",
    "    print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "\n",
    "for i in cost_vals_combos:\n",
    "    C1 = i[1]\n",
    "    C2 = i[0]\n",
    "    hlr = LogisticRegression(eta=eta, optimization_technique=\"Newton's method\", iterations = iterations, regularization=\"both\", C1=C1, C2=C2)\n",
    "    hlr.fit(X_train_scaled, y_train)\n",
    "    yhat = hlr.predict(X_train_scaled)\n",
    "    if accuracy_score(y_train,yhat)*100 > best_accuracy:\n",
    "        best_accuracy = accuracy_score(y_train,yhat)*100\n",
    "        best_C1 = C1\n",
    "        best_C2 = C2\n",
    "    print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "    \n",
    "print(\"Best accuracy with C1=\" + str(best_C1) + \" and C2=\" + str(best_C2) + \": \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326aa0d3-e46a-4e3b-b30a-fb6951cd0a46",
   "metadata": {},
   "source": [
    "The best accuracy score when applying both L1 and L2 regularization had these parameters:\n",
    "\n",
    "eta = 1\n",
    "\n",
    "iterations = 5\n",
    "\n",
    "C1 = 0.7196856730011514\n",
    "\n",
    "C2 = 5.179474679231207.\n",
    "\n",
    "The resulting accuracy score was about 52.55%. This is not great in difference compared to applying only L1 or only L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2876cd83-ae1f-4ce1-afe4-31ae170be018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C1=0.7196856730011514 and C2=5.179474679231207: 52.54668930390493%\n",
      "Accuracy of C1=0.7196856730011514 and C2=5.179474679231207: 50.50847457627119%\n",
      "2.0382147276337363\n",
      "CPU times: user 133 ms, sys: 392 ms, total: 525 ms\n",
      "Wall time: 36 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "start = time.time()\n",
    "eta = 1\n",
    "iterations = 5\n",
    "C1 = 0.7196856730011514\n",
    "C2 = 5.179474679231207\n",
    "\n",
    "hlr = LogisticRegression(eta=eta, optimization_technique=\"Newton's method\", iterations = iterations, regularization=\"both\", C1=C1, C2=C2)\n",
    "hlr.fit(X_train_scaled, y_train)\n",
    "yhat_train = hlr.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train,yhat_train)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_train,yhat_train)*100) + \"%\")\n",
    "\n",
    "yhat_test = hlr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test,yhat_test)*100\n",
    "print(\"Accuracy of C1=\" + str(C1) + \" and C2=\" + str(C2) + \": \" + str(accuracy_score(y_test,yhat_test)*100) + \"%\")\n",
    "print(abs(train_accuracy-test_accuracy))\n",
    "accuracies.append(test_accuracy)\n",
    "end = time.time()\n",
    "times.append(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5f672-a776-4cdf-9833-613609d4bec1",
   "metadata": {},
   "source": [
    "Using these parameters, the difference in accuracy scores was about 2.04%. This is comparable to those of applying only L1 and only L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "43abbd95-4133-4507-88b1-42b89658157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "Total number of failed predictions: 559\n",
      "Percentage of failed predictions for training set: 47.45331069609507\n",
      "Percentage of class 1 prediction failure in the entire training set: 15.365025466893039\n",
      "Percentage of class 1 prediction failure: 32.379248658318424\n",
      "Percentage of class 2 prediction failure in the entire training set: 15.025466893039049\n",
      "Percentage of class 2 prediction failure: 31.663685152057248\n",
      "Percentage of class 3 prediction failure in the entire training set: 17.062818336162987\n",
      "Percentage of class 3 prediction failure: 35.95706618962433\n",
      "Total number of successful predictions: 619\n",
      "Percentage of successful predictions for training set: 52.54668930390493\n",
      "Percentage of class 1 prediction success in the entire training set: 26.400679117147707\n",
      "Percentage of class 1 prediction success: 50.24232633279483\n",
      "Percentage of class 2 prediction success in the entire training set: 6.8760611205432935\n",
      "Percentage of class 2 prediction success: 13.08562197092084\n",
      "Percentage of class 3 prediction success in the entire training set: 19.269949066213922\n",
      "Percentage of class 3 prediction success: 36.67205169628433\n",
      "TESTING SET\n",
      "Total number of failed predictions: 146\n",
      "Percentage of failed predictions for testing set: 49.49152542372882\n",
      "Percentage of class 1 prediction failure in the entire testing set: 15.932203389830507\n",
      "Percentage of class 1 prediction failure: 32.19178082191781\n",
      "Percentage of class 2 prediction failure in the entire testing set: 18.64406779661017\n",
      "Percentage of class 2 prediction failure: 37.67123287671233\n",
      "Percentage of class 3 prediction failure in the entire testing set: 14.915254237288137\n",
      "Percentage of class 3 prediction failure: 30.136986301369863\n",
      "Total number of successful predictions: 149\n",
      "Percentage of successful predictions for training set: 50.50847457627119\n",
      "Percentage of class 1 prediction success in the entire training set: 30.508474576271187\n",
      "Percentage of class 1 prediction success: 60.40268456375839\n",
      "Percentage of class 2 prediction success in the entire training set: 6.779661016949152\n",
      "Percentage of class 2 prediction success: 13.422818791946309\n",
      "Percentage of class 3 prediction success in the entire training set: 13.220338983050848\n",
      "Percentage of class 3 prediction success: 26.174496644295303\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "total_success = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "class_1_prediction_success_count = 0\n",
    "class_2_prediction_success_count = 0\n",
    "class_3_prediction_success_count = 0\n",
    "\n",
    "for i, j in zip(y_train,yhat_train):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "    if i == j:\n",
    "        total_success += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_success_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_success_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_success_count += 1\n",
    "\n",
    "print(\"TRAINING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for training set: \" + str(total/len(y_train)*100))\n",
    "print(\"Percentage of class 1 prediction failure in the entire training set: \" + str(class_1_prediction_fail_count/len(y_train)*100))\n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure in the entire training set: \" + str(class_2_prediction_fail_count/len(y_train)*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure in the entire training set: \" + str(class_3_prediction_fail_count/len(y_train)*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))\n",
    "print(\"Total number of successful predictions: \" + str(total_success))\n",
    "print(\"Percentage of successful predictions for training set: \" + str(total_success/len(y_train)*100))\n",
    "print(\"Percentage of class 1 prediction success in the entire training set: \" + str(class_1_prediction_success_count/len(y_train)*100))\n",
    "print(\"Percentage of class 1 prediction success: \" + str(class_1_prediction_success_count/total_success*100))\n",
    "print(\"Percentage of class 2 prediction success in the entire training set: \" + str(class_2_prediction_success_count/len(y_train)*100))\n",
    "print(\"Percentage of class 2 prediction success: \" + str(class_2_prediction_success_count/total_success*100))\n",
    "print(\"Percentage of class 3 prediction success in the entire training set: \" + str(class_3_prediction_success_count/len(y_train)*100))\n",
    "print(\"Percentage of class 3 prediction success: \" + str(class_3_prediction_success_count/total_success*100))\n",
    "\n",
    "total = 0\n",
    "total_success = 0\n",
    "class_1_prediction_fail_count = 0\n",
    "class_2_prediction_fail_count = 0\n",
    "class_3_prediction_fail_count = 0\n",
    "class_1_prediction_success_count = 0\n",
    "class_2_prediction_success_count = 0\n",
    "class_3_prediction_success_count = 0\n",
    "\n",
    "for i, j in zip(y_test,yhat_test):\n",
    "    if i != j:\n",
    "        total += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_fail_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_fail_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_fail_count += 1\n",
    "    if i == j:\n",
    "        total_success += 1\n",
    "        if i == 1:\n",
    "            class_1_prediction_success_count += 1\n",
    "        elif i == 2:\n",
    "            class_2_prediction_success_count += 1\n",
    "        elif i == 3:\n",
    "            class_3_prediction_success_count += 1\n",
    "\n",
    "print(\"TESTING SET\")\n",
    "print(\"Total number of failed predictions: \" + str(total))\n",
    "print(\"Percentage of failed predictions for testing set: \" + str(total/len(y_test)*100))  \n",
    "print(\"Percentage of class 1 prediction failure in the entire testing set: \" + str(class_1_prediction_fail_count/len(y_test)*100))\n",
    "print(\"Percentage of class 1 prediction failure: \" + str(class_1_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 2 prediction failure in the entire testing set: \" + str(class_2_prediction_fail_count/len(y_test)*100))\n",
    "print(\"Percentage of class 2 prediction failure: \" + str(class_2_prediction_fail_count/total*100))\n",
    "print(\"Percentage of class 3 prediction failure in the entire testing set: \" + str(class_3_prediction_fail_count/len(y_test)*100))\n",
    "print(\"Percentage of class 3 prediction failure: \" + str(class_3_prediction_fail_count/total*100))\n",
    "print(\"Total number of successful predictions: \" + str(total_success))\n",
    "print(\"Percentage of successful predictions for training set: \" + str(total_success/len(y_test)*100))\n",
    "print(\"Percentage of class 1 prediction success in the entire training set: \" + str(class_1_prediction_success_count/len(y_test)*100))\n",
    "print(\"Percentage of class 1 prediction success: \" + str(class_1_prediction_success_count/total_success*100))\n",
    "print(\"Percentage of class 2 prediction success in the entire training set: \" + str(class_2_prediction_success_count/len(y_test)*100))\n",
    "print(\"Percentage of class 2 prediction success: \" + str(class_2_prediction_success_count/total_success*100))\n",
    "print(\"Percentage of class 3 prediction success in the entire training set: \" + str(class_3_prediction_success_count/len(y_test)*100))\n",
    "print(\"Percentage of class 3 prediction success: \" + str(class_3_prediction_success_count/total_success*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135b091-da6f-4e23-80a6-622446b41a29",
   "metadata": {},
   "source": [
    "In these calculations, we added visualizing the percentage prediction failure for classes out of the entire training and testing sets. Additionally, we wanted to visualize the success percentages for each class in both training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9b73f-13d4-4f40-85ba-7a3238eae0d6",
   "metadata": {},
   "source": [
    "##### 2.2.4.4 Analyzing Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e76b43-8dbe-443e-bb11-a30783dc3378",
   "metadata": {},
   "source": [
    "Newton's method did not show much improvement in consideration to the other two optimization techniques. This gives us some information that our features do not have a great deal of correlation with our target values. At the very minimum, they do not correlate enough to be predicted with a high percentage of accuracy using logistic regression. We will now use scikit-learn's built in logistic regression methods to test on our training and testing sets. Hopefully, this will confirm our current opinions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903f0f4-ea73-454b-b4c7-bfc7d1d1bfea",
   "metadata": {},
   "source": [
    "### 2.3 Scikit-Learn Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43958844-d487-4a6a-b539-a579b535d0ee",
   "metadata": {},
   "source": [
    "Now, we will use scikit-learn's built in logistic regression classifier. We first tested setting the solver to liblinear. We tested without regularization, with L1 regularization, and with L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b14b2350-c9e9-4ad5-8b3d-73127b09d56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.35477725  0.63832983 -0.49753209 -0.06015499 -0.79124315  0.18866761\n",
      "  -0.03109815 -0.0599003  -0.26027798  0.13452142]\n",
      " [-1.48364034  0.11303087  0.66451554 -0.04379945  0.3494207  -0.15455751\n",
      "  -0.09527249 -0.1382137   0.18814758 -0.04469239]\n",
      " [-0.64072895 -0.79918499  0.07645683  0.09694921  0.57302682 -0.06959829\n",
      "   0.09369364  0.16811316  0.14734959 -0.1322826 ]]\n",
      "Accuracy: 52.03735144312394%\n",
      "Accuracy: 50.847457627118644%\n",
      "CPU times: user 94 ms, sys: 412 ms, total: 506 ms\n",
      "Wall time: 35 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "start = time.time()\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='liblinear') # all params default\n",
    "\n",
    "lr_sk.fit(X_train_scaled,y_train)\n",
    "print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X_train_scaled)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "yhat = lr_sk.predict(X_test_scaled)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_test,yhat)*100) + \"%\")\n",
    "accuracies.append(accuracy_score(y_test,yhat)*100)\n",
    "end = time.time()\n",
    "times.append(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0e1d0cf-bfbd-4b4a-8f54-214205d6ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3519877   0.63399908 -0.49609859 -0.0560014  -0.7855498   0.18503827\n",
      "  -0.02739297 -0.05355274 -0.25684971  0.13172361]\n",
      " [-1.48321511  0.11226463  0.6597218  -0.02755048  0.34618776 -0.15086096\n",
      "  -0.09098203 -0.13257788  0.18396608 -0.03436188]\n",
      " [-0.63739099 -0.79335207  0.07249655  0.09422472  0.5661072  -0.065744\n",
      "   0.08994641  0.16169349  0.1440586  -0.12929915]]\n",
      "Accuracy: 52.03735144312394%\n",
      "Accuracy: 50.50847457627119%\n",
      "CPU times: user 25.1 ms, sys: 27.8 ms, total: 52.9 ms\n",
      "Wall time: 6.51 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_sk = SKLogisticRegression(penalty='l1', solver='liblinear') # all params default\n",
    "\n",
    "lr_sk.fit(X_train_scaled,y_train)\n",
    "print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X_train_scaled)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "yhat = lr_sk.predict(X_test_scaled)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_test,yhat)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ff8229b0-6f4b-4b08-9525-b1f71bc01e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.35477725  0.63832983 -0.49753209 -0.06015499 -0.79124315  0.18866761\n",
      "  -0.03109815 -0.0599003  -0.26027798  0.13452142]\n",
      " [-1.48364034  0.11303087  0.66451554 -0.04379945  0.3494207  -0.15455751\n",
      "  -0.09527249 -0.1382137   0.18814758 -0.04469239]\n",
      " [-0.64072895 -0.79918499  0.07645683  0.09694921  0.57302682 -0.06959829\n",
      "   0.09369364  0.16811316  0.14734959 -0.1322826 ]]\n",
      "Accuracy: 52.03735144312394%\n",
      "Accuracy: 50.847457627118644%\n",
      "CPU times: user 4.61 ms, sys: 0 ns, total: 4.61 ms\n",
      "Wall time: 3.79 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_sk = SKLogisticRegression(penalty='l2', solver='liblinear') # all params default\n",
    "\n",
    "lr_sk.fit(X_train_scaled,y_train)\n",
    "print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X_train_scaled)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "yhat = lr_sk.predict(X_test_scaled)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_test,yhat)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc8090-4519-4c08-87c1-7217d0f51c3d",
   "metadata": {},
   "source": [
    "Next, we tested using stochastic average gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c92a0de9-c52a-4cf6-bf46-19b1cd88b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.31236052  0.38445169 -0.38487714 -0.02765408 -0.53306845  0.13068631\n",
      "  -0.00624476 -0.02462933 -0.18337916  0.08230202]\n",
      " [-0.47590387  0.10989566  0.43099517 -0.03616301  0.21640194 -0.09427885\n",
      "  -0.06132166 -0.08839852  0.12719769 -0.0173734 ]\n",
      " [ 0.16354335 -0.49434735 -0.04611803  0.06381709  0.31666651 -0.03640746\n",
      "   0.06756642  0.11302784  0.05618146 -0.06492861]]\n",
      "Accuracy: 52.29202037351443%\n",
      "Accuracy: 50.847457627118644%\n",
      "CPU times: user 11.9 ms, sys: 594 µs, total: 12.5 ms\n",
      "Wall time: 11.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "start = time.time()\n",
    "lr_sk = SKLogisticRegression(solver='sag') # all params default\n",
    "\n",
    "lr_sk.fit(X_train_scaled,y_train)\n",
    "print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X_train_scaled)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "yhat = lr_sk.predict(X_test_scaled)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_test,yhat)*100) + \"%\")\n",
    "accuracies.append(accuracy_score(y_test,yhat)*100)\n",
    "end = time.time()\n",
    "times.append(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb7bea6-9632-4451-bb9e-d7b76f94e6c2",
   "metadata": {},
   "source": [
    "Lastly, we tested using Newton's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f146071a-e491-41a0-a048-3fe258cda4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.31236328  0.38445502 -0.38487564 -0.02766479 -0.53307215  0.13066703\n",
      "  -0.00625193 -0.02464225 -0.18337238  0.08231627]\n",
      " [-0.47591408  0.10989459  0.43099544 -0.03615623  0.2163963  -0.09427259\n",
      "  -0.06132449 -0.08839145  0.12719799 -0.01736401]\n",
      " [ 0.1635508  -0.49434961 -0.0461198   0.06382102  0.31667585 -0.03639444\n",
      "   0.06757642  0.11303369  0.05617438 -0.06495226]]\n",
      "Accuracy: 52.29202037351443%\n",
      "Accuracy: 50.847457627118644%\n",
      "CPU times: user 11.4 ms, sys: 0 ns, total: 11.4 ms\n",
      "Wall time: 10.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "start = time.time()\n",
    "lr_sk = SKLogisticRegression(solver='newton-cg') # all params default\n",
    "\n",
    "lr_sk.fit(X_train_scaled,y_train)\n",
    "print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X_train_scaled)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_train,yhat)*100) + \"%\")\n",
    "yhat = lr_sk.predict(X_test_scaled)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_test,yhat)*100) + \"%\")\n",
    "accuracies.append(accuracy_score(y_test,yhat)*100)\n",
    "end = time.time()\n",
    "times.append(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa866e2-b768-4f90-8e91-7d5733806aa5",
   "metadata": {},
   "source": [
    "None of the accuracy scores from using scikit-learned performed better than our own implementation to a significant degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366986bf-020e-4faa-8790-988e921d4ec3",
   "metadata": {},
   "source": [
    "### 2.4 Compare Custom Implementation Performance with Scikit-Learn Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bb73d-f594-404b-a037-529acbd7bac7",
   "metadata": {},
   "source": [
    "Obviously, scikit learn implementation performed better in regards to training time. The times for using scikit learn ranged from 22 to 72 ms. For our own implementation, applying steepest ascent and stochastic gradient ascent usually took at least 1 second. Our implementation of Newton's method did train and predict in about 3 ms.\n",
    "\n",
    "In regars to accuracy there really is not much variance. The highest level of prediction accuracy never surpassed 54% for both our own and scikit-learn's implementation. The prediction accuracies for the training set usually ranged from 50-53%. The prediction accuracies for the testing set usually rnaged from 48-51%.\n",
    "\n",
    "There was no reason to visualize since none of the accuracies met our standards. Additionally, the difference between them was very negligible. However, using scikit learn always produced the same results. Thus, we knew what the results would be with more certainty. In comparison to our own implementation, the results varied everytime we ran the code and applied a new fit regardless of using the exact same parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379c412-450c-43bb-9dd0-230701f2db2c",
   "metadata": {},
   "source": [
    "## 3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a5f21-5d0f-4dc8-ba64-f7667941bd20",
   "metadata": {},
   "source": [
    "Our testing demonstrated that there is not as much correlation between the features and the target values. Thus, all our various implementations of logistic regression were only showing roughly 50% prediction accuracy on average for both training and testing sets. This was also seen when using scikit-learn. This is not nearly good enough for the third parties that would want to use our model. We could not with good faith allow any governments or health care institutions to use any of our implementations on this dataset.\n",
    "\n",
    "Thus, we do not advise deploying any implementation, our own or scikit-learn, of logistic regression on this dataset. In the future, we might want to take different combinations of the features to use as input for our training and testing sets. Then, rerun all of our algorithms on these various combinations. It might be interesting to see if there is a certain mix of features that does correlate to the target values at least seen by using logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df134d78-b15f-4887-86ee-592bfef00170",
   "metadata": {},
   "source": [
    "## 4. Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4e7a40df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "methods_list = [\"Steepest\\nAscent\", \"Stochastic\\nGradient\\nAscent\", \"Newton's\\nMethod\", \"SKLearn\\nLIBLINEAR\", \"SKLearn\\nSAG\", \"SKLearn\\nNewton's\"]\n",
    "# plt.xticks([])\n",
    "plt.bar(methods_list, accuracies)\n",
    "plt.xlabel(\"Classifiers\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# https://www.geeksforgeeks.org/how-to-display-the-value-of-each-bar-in-a-bar-chart-using-matplotlib/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "27aaa75a-816f-4d41-9a50-738d07080b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(methods_list, times)\n",
    "plt.xlabel(\"Classifiers\")\n",
    "plt.ylabel(\"Performance Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609c38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0358e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0441cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
